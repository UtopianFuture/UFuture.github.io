## Media

把手机芯片的架子搭好后，需要看看怎么加入多媒体部分。

所谓多媒体，包含三个模块：图形处理器(GPU)，显示模块(Display)，视频模块(Video)。显示模块负责把所有的内容输出到屏幕，视频模块负责解码片源，也负责编码摄像头的录制内容。图像信号处理（ISP）模块暂时不算在内，以后另说。

GPU 是大家喜闻乐见，津津乐道的部分，各种跑分评测都会把 GPU 性能重点考量。但是实际上，在定义一个手机芯片多媒体规格的时候，我们首先要确定的参数，不是 GPU 有多强大，而是显示输出的分辨率：是 720p，1080p，2K 还是更高。这个参数，决定了 GPU 填充率(fill rate)的下限，系统带宽大小，内存控制器的数量，还会影响 CPU 和 ISP 的选择，从而决定整体功耗及成本。所以，这一参数至关重要。

举几个典型的例子：

超低端：展讯的 SC9832，显示分辨率 720p，ARM Cortex-A7MP4，Mali400MP2, 1xLPDDR3

低端：联发科的 MT6739，显示分辨率 1444x720，ARM Cortex-A53MP4@1.5GHz，IMG PowerVR GE8100，1xLPDDR3

中端：高通的骁龙 652，显示分辨率 2560x1600，Cortex A72MP4/Cortex A53MP4，Adreno 510，2xLPDDR3

高端：海思的麒麟 970，显示分辨率不高于 2K，Cortex A73MP4/Cortex A53MP4，G72MP12，4xLPDDR4

我们可以看到，随着显示分辨率上升，芯片规格越来越高，但到 2K 就停止了。下面通过定量分析，让我们看看其中每个参数背后的考量。

![img](https://pic2.zhimg.com/80/v2-e52e5bc66310f00a13f61e7f08e8ed3d_1440w.webp)

如上图所示，手机多媒体一定包含图形，视频和显示三个模块。为什么桌面图形处理器囊括了视频和显示输出，而手机要把它们分开？再极端一点，其实所有的多媒体和图像处理都是计算，为什么不全用 CPU 做了？把省下来的面积全做成 CPU，岂不更好？不好意思，这不可行。原因很简单，功耗。请记住，由于没有风扇，手机芯片无论怎么设计，在各种长时间运行场景下，功耗**一定得低于 2.5 瓦**，短时间运行也不宜超过 5 瓦，瞬时运行倒是可以更高。这个 2.5 瓦，除了跑多媒体，还得包括 CPU，总线和内存带宽。而多媒体的每个模块，在做其擅长的事情时，功耗远低于 CPU。16nm 上视频编解码器在处理 4K30FPS 帧的功耗在 60 毫瓦左右，而相同的事情让 CPU 做，我粗略的估计了下，至少得四个跑在 2Ghz 的 A53，还得是 NEON 指令(功耗为 Dhrystonex2.5)，那就是 1.5 瓦以上的功耗。相差 25 倍。

再看显示模块的功耗，分辨率 2K60 帧下，16nm 工艺需要 50 毫瓦左右。而用 GPU 做相同的事情，粗略的算，需要 300 毫瓦左右。用 CPU 还要乘以 2 到 3，近 1 瓦。

所以，只是放个 4K 的视频并输出到屏幕，就已经到了功耗上限了，还没有计算访存功耗呢，更不用说支持 10 小时以上的视频播放。所以，手机多媒体必须把 GPU，视频和显示模块分化出来。

当然，如果手机非常低端，一定要用 CPU 来进行软件解码，从而省了硬件面积，也不是完全不可行。因为超低端可能只要支持 1080p 视频就可以了，并且由于 CPU 数量小还是小核，功耗虽然高些但也不是完全不能接受。同时，低端的手机 CPU 本身处理能力弱，软件优化和多核负载均衡一定要做好。而中高端手机不会为了省面积这么做的。

看到这可能有人会问，为什么视频是 4K 的，而显示只有 2K 呢？分辨率不匹配，多出来的像素不是浪费么？确实如此。不过由于受到手机屏的限制，目前就算高端手机也还没支持 4K。并且屏幕分辨率提高一倍，功耗也提高一倍。这对于本就是耗电大户的屏幕来说，是个大问题，要解决就等着以后更低功耗的屏幕出现了。

反过来，为什么不把视频解码降到 2K 呢？那是因为视频源的格式是片源决定的，4K 的片源没法用 2K 的解码器去解，只能解完再降分辨率。

还有一个问题，为什么上文中视频是 30 帧，而显示是 60 帧呢？我曾经做过实验，特意把屏幕刷新率改成 30，结果完全没发现什么不同。但是，据说大部分人的眼神比较好，动态视觉强，对于非自然图像很敏感，所以对于手机背景等图像，一定要做到 60 帧才能感到流畅。而对于自然图像，比如看视频，30 帧就感到流畅了。所以，这两类刷新率就约定成俗了。

下面，我们来看下，屏幕分辨率是如何影响 GPU，系统带宽以及内存控制器的。

显示模块的任务和操作系统的用户界面(UI)中图层的概念有关系。我们看到的最终屏幕画面就是多层图层合成叠加的。同时，显示模块还可以对每一层进行旋转，缩放等操作，最终生成一幅图，转成所需的信号格式输出。显示模块的输入可以是解码后的视频，也可以是 GPU 丢过来的完成初步合成的图层。上图中，显示模块支持 3 路输入，外加背景图输入，我们一般关注前者。

以安卓为例，用到的图层一般在 4-8 层。假设显示是 1080p60fps，那每一层的带宽就是 1920x1080x60x4(RGBA)=480MB/s，8 层就是 4GB/s。这是系统给显示模块的输入，总线上还得有输出。假设这时候再播放 4K30fps 视频，所需带宽未压缩是 1.2Gx1.25=1.5GB/s。

而 GPU 跑用户界面时的典型带宽开销，根据 UI 的复杂度不同，每 60 帧需要的带宽可达到 1GB/s（压缩后），没压缩时在 1.5-2GB/s。其他的还有 CPU 跑驱动，APP 等开销，加一起算 1GB/s 的话，总共 9GB/s 左右。

单通道的 LPDDR4 带宽大致在 12.8GB/s，如果带宽利用率 70%，那差不多正好用满一个 DDR 控制器，此时每 GB 带宽消耗在 DDR PHY 的功耗是 100 毫瓦（16nm），加上总线和 DDR 控制器的功耗，总共需要 1 瓦左右。

这里我们可以算出来，除了 CPU/GPU/Video/Display 之外，带宽也非常费电，而且增加带宽会较明显的增加内存控制器，DDR PHY 和内存颗粒数量，成本上升。相应增加的总线面积和功耗到相对并不大，可以忽略。至于解决由此带来的复杂度，那是设计 SoC 的基本功，架构篇提过，这里不再重复。

至此，我们已经可以看到如何由显示分辨率反推对于系统带宽，功耗和成本的需求。而 GPU 的最小需求也可以由此推导出来。

定义 GPU 有三个重要参数，三角形输出率，像素填充率和理论浮点性能。对用户界面来说，意义最大的是像素填充率。

填充率有什么意义？对于上文提到过的每层图层，如果分辨率是 1080p，那就需要 1920x1080x60=120M/s 的像素填充率。如果 8 个图层全部由 GPU 画出，那么就需要 1G/s 的填充率，对应上图的 MP2。这还不止。还记得显示模块里面的合成，缩放和旋转功能吗？这些其实 GPU 也能做。如果显示模块能力不够，只支持 4 路输入，那我们就需要 GPU 把 8 层图层先合并为 4 层，然后才能交给显示模块。每两层合成相当于重画一层，于是又额外的需要 4 层，共 1440M/s 的像素。如果还涉及缩放和旋转，那还需要更多。通常来说，显示模块不会支持到 8 层，因为这样的场景并不多，会造成硬件冗余。而极端场景下，GPU 就被用来完成额外工作，增加灵活性，又能防止屏幕因图层过多造成的卡顿。

当然，由于系统延迟和带宽的存在，像素利用率不可能达到 100%.之前的几年,我看到的有些系统只能做到 70%的利用率,主要原因是平均延迟太长,而并行度不够大.这时候,简单的增加 GPU 核心数量并不是一个明智选择,并且如果瓶颈是在系统带宽不够,或者系统调度没做好,即使增加像素输出率也无济于事。近两年的手机芯片基本上可以做到 90%的利用率.但是,就算是低端手机,还是会留出更多的填充能力,来应付多图层下复杂操作的突发情况.此时,提高利用率的意义就成了减小功耗.

此外，在很多移动 GPU 上，像素填充率还意味着同等的材质填充率。因为用户界面基本都是拿图片或者材质来贴图然后混合，不需要大量计算三维图形，三角形输出和浮点能力用处不大，但是材质填充率必须匹配。

按照上文的功耗和面积，下面我们来看两个极端的例子：

支持 VR 的芯片，显示分辨率 4K120fps（双眼），在虚拟房间内播放 4k 视频，显示模块支持 8 路输入，那么就可能需要 4x1080x1920x120x7=6.4G/s 的像素填充率，外加一路 4k 视频解码。换成 GPU 就是至少 G72MP8，而考虑 3D 性能，MP16 都是不够的。仅仅 GPU 部分的面积就要几十平方毫米，功耗 5 瓦以上，系统不加风扇没法跑。

低端的芯片，仅支持 1080p，4k30 帧播放视频，4 层场景，一个 GPU 核就能搞定，面积 2 平方毫米，功耗 0.5 瓦不到。加上视频和显示模块也不会超过 5 个平方毫米，功耗之前我们也算过，较低。

这里还没有考虑 GPU 驱动对 CPU 的需求。满负载的话，G72MP12 就需要一个 A73 跑满 2.5Ghz 且很难均衡负载（OpenGL ES 的限制），而低端芯片只需一个 A53 就轻松完成。这里面大核小核，4 核 8 核又造成了非常大的面积和功耗区别。

所以，提升显示分辨率绝不仅仅是图像细致一些这么简单，提升一倍的话，系统成本和功耗基本也会上一大截。简单来说，显示分辨率决定了一个芯片的下限。

把 GPU 在系统中的基本角色介绍完，下面从设计 GPU 的角度来分析。

想要做好一款 GPU，先要分析市场。GPU 市场主要有四大块：桌面和游戏机（3 亿颗以下），手机和平板（20 亿颗以下，其中近 15 亿颗被高通和苹果占住），电视和机顶盒（2 亿颗以下），汽车面板和自动驾驶（小于 1 亿颗）。其中桌面和游戏机，自动驾驶暂不考虑，其他几类需求如下：

手机和平板：显示分辨率 1080p 到 2K，4-8 层图层，3D 性能从弱到强，功耗 2.5 瓦，成本敏感。

电视和机顶盒：显示分辨率 1080p 到 8K，8 层图层，3D 性能弱，功耗 2.5 瓦，成本敏感，需要画质增强。

汽车面板：显示分辨率 1080p 到 2K，4 层图层，3D 性能弱，功耗 2.5 瓦，成本较敏感，不太需要汽车安全设计。

由于 Vulkan 成为安卓的下一代图形接口，固定图形流水线设计必将退出舞台，通用图形处理器，也就是所谓的 GPGPU，成为必然趋势。

分辨率的变化可以提炼为可配置多核设计，UI 和游戏的不同需求可提炼为大小核的设计。这里，大小核代表着同样填充率下不同的计算能力。两者结合，以期达到最高能效比和面积比。说到大小核，自然就衍生出一个问题，有没有必要像 CPU 那样在一个芯片内集成 GPU 的大小核？答案是否定的。CPU 大小核之所以有用，是因为能效比会有 4 到 5 倍的差别，以及单线程性能的硬需求。而一个好的 GPU 设计，大小核无论跑 UI 还是图形，由于存在天然的多线程属性，**同样的性能所消耗的能量应该是一致的**。大小核面积会有差别，但是即使某段时间只用 UI，不用计算能力，也得把计算能力放在芯片里，所以小核的意义就不大了，除非就是以 UI 为主要应用场景的 GPU，不追求 3D 性能。

渲染方式上，目前主要有即时渲染和块渲染，这个话题已经有些年头了。前者是按照图元为基准，渲染相关顶点，几何，像素，然后合成输出。后者是以像素为基准，选取相关顶点和三角形，计算覆盖关系，最终合成输出。初一看，块渲染似乎更经济，因为它可以计算像素覆盖关系，避免重复渲染。但是反过来，块渲染时所需的三角形，顶点，属性和 Varying 信息，都是需要从内存读取的。如果存在大量的三角形，就需要多次重复读取，很可能省掉的带宽还不如用即时渲染。所以，决定哪个方式更优，关键在于顶点和像素的比例。就目前手机上的应用看来，像素远大于三角形或者顶点数量，这个比例大致在 50：1 到 30：1。这时，用块渲染就更适合嵌入式设备。

从计算密度看，即时渲染的 GPU 面积一定小于块渲染的 GPU，但是增加了带宽，变相增加了手机成本。至于增加的功耗，未必会比块渲染方式多。所以定性的讨论还是不够的，需要经过定量计算才能确定。

有个例子：某即时渲染的 GPU A 和块渲染的 GPU B，填充率和曼哈顿 3.0 跑分比例一致，两个 GPU 都相差 5.5 倍。GPU A 功耗低了 30%，面积只有一半，但是带宽却是 3.4 倍。

这 3 倍多的带宽，几乎占了 1.5 个 DDR4 通道，并一下子带来了 2 瓦的功耗（28 纳米），之前的 GPU 自身功耗优势当然无存，哪怕面积小一半也无济于事。

反过来，如果按照 GPU B 的绝对性能，那 GPU A 其实只需要 2.3GB/s 的带宽，虽然很大，却远不到一个 DDR4 通道最大带宽，同时功耗也很低，达不到 2.5 瓦的功耗上限，还能省一半面积，何乐而不为呢？

由此可以得出结论，**低端手机完全可以用即时渲染的 GPU，而中高端上还是得使用块渲染的 GPU**。随着工艺进步，即时渲染的 GPU 适用范围会更广。这应该出乎很多人的意料。

关于带宽还有一点，和 CPU 的延迟敏感不同，GPU 是靠增加并行度来提高效率的。CPU 的预取机制，虽然缩短了延迟，但是增加了带宽和功耗，对与带宽吃紧的 GPU 来说并不合适。所以 GPU 的缓存并不适合预取机制。

接着我们来看看图形渲染的流程：

![img](https://pic1.zhimg.com/80/v2-18acec572997da8c0cb074acdb2997d0_1440w.webp)

每一步的过程不具体解释，我们关心的是哪些可以用通用计算单元做，哪些还是要固化为硬件做，这和性能面积功耗强相关。我们把上图流水对应到 GPU 上，如下图：

![img](https://pic1.zhimg.com/80/v2-48d931fcb385f23c25715d5971888030_1440w.webp)

其中，顶点和像素的处理，计算量相对大，算法相对变化大，可以用通用的着色器，也就是上图中的执行单元 Execution Engine。

曲面细分模块 Tessellation，由于并不是必须的，在 Mali 的 Norr 中，并没有对应的硬件模块，可以用软件使用着色器通用处理单元来做。

深度和模板的测试以及合成，这些都属于像素的后处理，可以用专用硬件直接做，因为操作简单，计算量也和像素线性相关。

材质需要一个额外的单元来做，因为材质有许多专用的操作，而且每个像素点的输出都需要材质单元参与，输出线性相关。此外材质访存带宽也很大，所以拥有自己的访存单元，不占用着色器的存取单元。

在顶点和像素计算中用来传递数据的属性和 Varying，需要根据顶点的属性数据，读取数据，插值计算像素的值，提供给像素着色器。计算量和顶点或者像素线性相关，适合固化为硬件单元。

根据图元的顶点位置信息，计算转换后的坐标与法向量，如果在边界之外或者在背面，那就不用输出，直接扔掉，节省带宽。这步就是背面剔除 Culling 和裁剪 Clipping，可以用专用模块配合通用计算单元办到。最后剩下的有效部分，可以用来生成三角形列表，并增加到基于块状像素的队列中去，以便于光栅化。

接下去就是光栅化。这一步中涉及到深度和颜色等的计算，用通用着色器来做。具体做的时候，还需要一个硬件的三角形设置模块，对于某一块像素区域所涉及的三角形，读取上一步中形成的三角形列表，计算三角形每条边所对应方程的参数，以及平面和重心。由于和三角形输出率相关，算法固定，所以也适合固化。

此外，还需要一些额外的单元，来处理系统相关的事务，比如内存子系统，内部总线，以及负责管理任务和线程的模块。其中，任务管理模块又可以在不同层面细分，是一个 GPU 设计的精华所在。

以 Mali 为例，在最上层，以图元，顶点和像素为基准，把所有的任务都划分成三类 Job，交给不同的处理单元，也就是 Tiler，顶点/像素着色器。下一层，按照像素块为单元，又可以把整个屏幕分成很多像素点，也就是线程。在同一瞬间，每个着色器上都可以跑多个线程，这些线程的组合又被称作一个 Warp。**而提高计算密度的秘密，就在于在一个核内塞入更多的线程数。**

再下层，具体到每一个着色器里面的执行引擎，每个时钟的输入是一个程序块 Clause。Clause 就是某段既没有分支也没有访存的程序，当所有的输入数据都已经从内存读取并存放于寄存器后，这段程序会被无间断执行，直到结果输出。执行过程中，临时寄存器会被用来解决通用寄存器不够用的问题。而当输入数据还未取到，那么就切换到另一个准备就绪的程序块。

![img](https://pic2.zhimg.com/80/v2-2ca26c9d45f657b86a1b994e376027a9_1440w.webp)

所有这些任务，Warp 和 Clause 的管理，需要有专门的硬件来做，以保持整个流水线利用率的最大化。

总之，只要是一直出现在图形流水上的工序，操作固定，计算量稳定，就可以用专用硬件单元来做，并不会浪费，相反还更省功耗面积。而计算变化较大的部分，就可以交给通用计算单元。

![img](https://pic1.zhimg.com/80/v2-3e51b2681430cbb3a68f2e1b08ba8e70_1440w.webp)

确定了通用和专用单元，接下来需要优化渲染流程，并调整硬件。先看顶点计算，如上图。在生成三角形列表的时候，有些三角形代表着背面，那只要算出法向量，那么就可以直接确定是否抛弃，省掉输出。

更进一步，如果根据顶点的远近关系，做某些简单计算，直接可以得出某些三角形被完全覆盖，那么也可以直接抛弃。在之后的像素渲染以及合成中，可以完全省掉处理。这被称作 Early-Z。不过这还存在一个限制，在多个绘制函数 Draw call 中，如果命令被先后发送到 GPU，不同函数间不容易做 early-Z 优化，因为如果要保留上一个同一区域绘制函数的结果一起做优化，可能需要花更大的代价。但是其块渲染的任务却可以等到所有绘制函数都完成后再开始。在这种情况下，从后往前画的绘制函数区，就不容易优化，而从前往后画的直接就能完成覆盖。这被称为 forward killing，需要图形引擎预先计算出物体的深度信息，在生成脚本阶段就做好预判，提高渲染效率。

以上优化并不能解决所有的三角形覆盖问题，还可以再进一步。在像素渲染阶段，得到像素点对应的三角形以及深度信息后，一样可以抛弃被遮住的三角形，只计算被看到的那个三角形的颜色和光照，纹理，同样也免了后续的混合。这一步中的操作被称为 TBDR，延迟块渲染。这是可以甚至是跨越 Draw call 的。如果顶点数足够少，被覆盖的三角形足够多，Early-Z 和 TBDR 可以极大的减少像素渲染计算量。

在合成阶段，我们还可以做一个优化，就是在输出最终的块内容时，对整个区域计算一个 CRC 值.如果是 16x16 的块，其 CRC 大小通常只有 1%，1080p 的屏幕是 100K 字节左右。在下一次渲染时，我们再从 DDR 甚至内部缓存读出这个 CRC 值，来判断是不是内容有变化。如果没有，那直接放弃输出，节省带宽。不过，之前所做的渲染计算还是没法节省。

如果知道屏幕有一块区域在一段时间内不会有内容变化，那我们可以预先就告诉 GPU，让它把这块区域从像素渲染里直接取消，从而免掉上一段中的计算。不过这需要和显示模块一起配合完成。

类似的渲染流水层面的优化还有很多，几乎每一步都能找出来。

在综合了所有的优化之后，我们终于做出了一个初始 GPU。其中计算引擎，面积 50%左右，占了一半，材质单元，面积大概在 20%，其余所有的加起来 30%。显然，优化的核心应该放在前两块。这其实又引出了图形处理器的一个奋斗目标：更高的计算密度。前面提到过，计算密度的定义，在以 UI 为主的低图形处理器上以像素输出率来衡量，在以游戏为主的高端上以浮点密度来衡量。

要提升 UI 像素输出密度，在合成单元能力固定的情况下，计算单元不重要，材质单元必须与输出能力匹配，一般是像素材质比 1：2 或者 1：1。1：2 的比例能做到两个材质点混合为一个后，配合一个像素输出，这在有些 UI 场景下很有用，提升了一倍的像素输出率。但是反过来，如果用不到，那多出来的材质单元面积就是浪费。具体是什么比例，只能见仁见智。

要提升浮点密度，方法也不难，就是堆运算单元，然后匹配上相应的图形处理器固化硬件，指令，缓存和带宽。

在具体设计运算单元的时候，还是有些考量的。之前，ARM 一直使用 SIMD+VLIW 的结构。也就是说，以一个像素为一个线程，以其 RGBA 四个维度为矢量，形成一个 32 位数据的 SIMD 指令。然后，尽量找出可以并行的 6 个线程，放到一起并行。这 6 个线程分别是向量乘，向量加，标量乘，标量加，指令跳转还有查表。其实就是对应了运算单元的设计，如下图：

![img](https://pic4.zhimg.com/80/v2-c5175993bb7367272a8dc15c28c6b6b7_1440w.webp)

由于 Mali 是基于块渲染的，一个块内有 16x16 个像素，也就是 256 个线程，这些线程可以处于不同的程序段，有些在计算深度，有些在计算颜色。如果线程管理器能够一直找到这样的 6 个像素，对应不同的运算单元，把这些单元一直排满，那自然可以得到最高的单元利用率。可惜事与愿违，这样的高利用率场景并不好找，很多的时候是只有矢量单元被用上了，其余的都空着。

于是，Mali 画风一转，把上图的标量乘和加去掉，且放弃 VLIW，从而把指令跳转单元抽出来。最后形成了一个新的处理单元，如下图：

![img](https://pic4.zhimg.com/80/v2-39f1d0ab94e849b3b20245bee220e797_1440w.webp)

这里，一个 128 位宽的乘加和同样宽度的加法单元承担了之前标量和向量乘加，而查表运算也和加法单元混合在一起。和之前完全不同的是，这里的输入始终是 128 位宽的单个指令，而不是 VLIW 的 6 条指令，从而提高计算单元利用率。每时钟周期进来的数据，只能到 FMA 和 ADD/TBL 单元中的一个，没法同时进去，某种程度上减少了面积的有效利用率。

![img](https://pic4.zhimg.com/80/v2-4a6ba06afc449ad8d6e18f2e15738d53_1440w.webp)

为了配合这一设计，Mali 还做出了一个新的调整，如上图。由之前的按照像素点的 RGBA 四通道的矢量运算方式，改成四个像素各抽取一个颜色通道，塞到上面的 FMA，同时运行四线程。由于大部分情况下，一个块上的 256 个像素的相同通道总是做一样的计算，保证了这个设计的高利用率。如果相邻四个点每个通道的运算都不一样，那效率自然会降低。

有些 GPU 还有另外一种运算单元形式：

![img](https://pic3.zhimg.com/80/v2-ce641c63a3cd9ce67ee2dde9aab612f6_1440w.webp)

在这里，乘加被放在小单元，比例更高；大单元除了乘加，还有查表，比例低；跳转单元单独放。这样可以使得计算单元比例更合理，面积利用率更高。

确定了计算单元的能力之后，有没有一个统一的方法，来精细的调整每个配套模块的比例呢？答案是有，先确定跑分标准，然后细化成子测试，最后在模型上统计出来：

目前移动上比较流行的标准是 GFXBench，主流的有三个版本，2.x，3.x 和 4.x。每一个版本都有侧重，比如 2.0 侧重三角形生成，3.0 侧重计算单元与材质，4.0 侧重计算。Antutu 也是一个标准，目前侧重阴影和三角形生成率。有时候，芯片和手机公司还会统计出主流的游戏，在芯片或者仿真平台甚至模型上跑，以期得到下一代 GPU 对计算能力的需求。

定下了标准跑分，接下去就是细化成更小的目标，是三角形，顶点，像素，材质，ZS，Varying，混合还是带宽要求。然后，在模型上，把这些细化需求翻译成 PPA，给到每一个小模块，看看是不是还有压缩的空间。这是最底层的优化。

经历过上面的打磨之后，我们得到了一个更好的 GPU。那是不是就没什么好改进了呢？还不够。从应用角度还是不停地有需求进来：

DRM，数据压缩， 系统硬件一致性，统一内存地址，AR/VR/AI，CPU 驱动。

首先，是版权保护，播放有版权的内容时，解密和解码都是在保护世界完成的，而 UI 的操作可能需要 GPU 的参与。这块在安全篇中有论述，此处不再展开。

第二，所有的媒体和材质数据都可以进行压缩，以节省系统带宽和成本。上文讨论过，此处不再展开。

第三，系统双向硬件一致性问题。要实现 GPU 和 CPU 以及加速器之间数据互访而不用拷贝和刷新缓存，就需要支持双向一致性的总线，比如 CCI550，这在基础篇已经讨论。最新的 OpenCL2.0 和 Vulkan 都支持这一新的特性。在数据交互非常频繁地情况下，可以节省 30%甚至 90%左右的运行时间。

不幸的是，由于 OpenGL ES 天生就不支持这个特性，所以对于目前绝大多数的图形应用，哪怕接了 CCI550，GPU 也是不会发出任何带有监听操作的传输的。这种情况到了 Vulkan 以后会有改善。

第四，和 CPU 的统一物理地址。在桌面上，CPU 和 GPU 的访问空间是完全独立的，而移动处理器从开始就统一了物理地址。当然，他们的页表还是分开的。在基础篇我们就讲过，统一的好处就是省带宽和成本，恰好块渲染的 GPU 的特点就是带宽相对较小。剩下的只要把总线和内存控制器的调度做好，保持内存带宽的利用率在一个相对较高的水平就行。

硬件一致性和统一地址有一个应用就是异构计算，CPU/GPU/DSP/加速器均可使用相同物理地址，并且硬件自动做好一致性维护。具体的计算可以是图像，也可以是语音。不过很可惜，在高端手机上，如果把所有的处理单元都跑起来，那功耗肯定是远高于 2.5 瓦的，甚至可以到 10 瓦。而且受限于 GPU 的软件，双向硬件一致性也没有得到广泛应用，目前最多是做一些 ISP 后处理。

第五，AI。在 AI 篇我们提到。如果 AI 跑在 GPU，那肯定需要支持 INT8 甚至更小的乘加操作，这对于 GPU 没有任何问题。不过，AI 更需要的参数压缩，Mali 的 GPU 并没有原生支持。这样一来，本来的四核 MP4 差不多是用 2 个 128 位 AXI 接口，只能提供 32GB/s 左右的读带宽。不压缩的话，也就能支持 64GB INT8 的计算量，远小于 GPU 四核一般在 1Tops 的 INT8 计算量。

第六，VR。VR 对于 GPU 来说有相当大的关系。首先，由于左右眼需要分别渲染，并且分辨率需要 4K 以上，这就对 GPU 性能提出了 1080p 时 8 倍的需求，对系统带宽也是一种考验。细分下来，有这样一些需求：

左右眼独立渲染：如下图，VR 场景中的三角形或者顶点部分，左右眼是共享的，但是旋转和之后的像素处理，必须是分开的。由于顶点渲染只占整个工作量的 10%，所以能节省的计算量相当有限，只是可以减少些顶点材质的读取。

![img](https://pic2.zhimg.com/80/v2-855dc7f83f20cf5a1ab7a0d1407f1bfd_1440w.webp)

畸变矫正：这个可以在顶点渲染最后加一步矩阵乘法，轻易做到。不过，由于这个操作夹杂在顶点和像素渲染之间，所以还是需要额外的 API 来提醒硬件。

![img](https://pic4.zhimg.com/80/v2-737a546a8bf85a1ec0f715ce74949ab7_1440w.webp)

异步时间扭曲 ATW：其原理是当发现计算下一帧需要的时间超出预期，索性就不去计算了，而是把当前帧按照头部移动方向做一个插值，造一个假的图像。这样，就需要一个 API，以估算下一帧生成时间，还需要一个额外的定时器，根据显示模块的 Vsync 信号计算剩余可用时间。如果时间不够，会直接拿取插值后的图像，而这个图像计算也在 GPU，计算量小且优先级很高，保证赶上 Vsync 信号。

![img](https://pic1.zhimg.com/80/v2-ee9eeab251a8c7738dfbe01c2bd8750c_1440w.webp)

多视图渲染 Multi-View：也就是对于视图的非焦点区域，使用低解析度，焦点区域，高解析度。要做到这点，使用高解析度，总的计算量可以降低一半左右。实际运用中，由于焦点区域的确定需要眼球跟踪，比较复杂，所以会采用中心区域来替代。

Front buffer：原来的桌面 GPU 设计中，显示缓冲分两块，front buffer 和 back buffer 交替输出，当中用 Vsync 做同步，按帧输出。现在只使用一块 front buffer，以 hsync 为同步标志，按行输出，这样，整个帧的渲染时间并不变，但是粒度变细。对于 GPU 来说，这需要加入按行渲染的次序关系。这和上一个 Multi-View 原理并不相同，前一个虽然分成了多视图区域，但是并不规定渲染次序，块与块之间还是乱序的，只不过最后输出的时候都是渲染完成好的。而 Front buffer 相当于在行与行间插入了一个同步指令，如果纯粹交由硬件来调度，很可能会降低性能。也可以 GPU 的 frame buffer 保持原样，用显示模块来做这个事情，更容易实现。

第七，AR。在 AI 篇中，我们提到，GPU 其实也在做渲染，没有什么特殊的需求，除非把识别的工作交给 GPU 来做。

还有很重要的一点，就是 GPU 驱动对 CPU 造成的负载。在 OpenGL ES 上，由于 API 本身的限制，很多驱动任务必须是一个线程内完成的。这就要求必须在一个 CPU 核上跑。GPU 核越多，单个 CPU 核的负载越高。在 Mali G71 之前，差不多 10-12 个 900Mhz 的 GPU 核就需要一个跑在 2.5Ghz 的 A73 来负责跑驱动，其他的大核小核再多都帮不上忙。但事实上，由于大核的能效比小核差了 4 到 5 倍，所以一定是小核来跑驱动更省电。反过来，如果使用了更多的 GPU 核，那最后的瓶颈会变成单个 CPU 的性能，而不是 GPU。解决的方法有几个，一是使用 Vulkan。Vulkan 在设计阶段就考虑到了这个问题，天然支持 CPU 多线程的负载均衡，能很好的解决这个问题。我曾经看到过一款桌面 GPU，在 16 核 A53 的服务器上只发挥出 x86 服务器上的 30%性能，而从 Open GL 换成 Vulkan 后，才把瓶颈转移到 GPU 自身。

不过，虽然 Vulkan 已经是谷歌钦定的下一代图形接口，基于 Vulkan 的图形应用流行可能还需要 3-5 年时间。另外一个可行方法就是优化 GPU 驱动软件本身，并把一部分的软件工作交给硬件来完成，比如内存管理模块的硬件化，还可以使用一个 MCU 和内嵌缓存来翻译和处理命令序列，替代 CPU 的工作。这个 MCU 可以只跑在几百兆，功耗十几毫瓦，远低于小核的 100 多毫瓦，更低于大核的几百毫瓦。

最后，做出来的 GPU 在 PPA 上还得和高通的 Adreno 对比（苹果的 GPU 在计算密度上也不如高通）。目前世界上所有的块渲染的 GPU 中，高通是能效比和性能密度最好的，计算密度比最新的 Mali GPU 高 30%以上，能效高 50%，目前已经是每个 Warp32 线程了。其中，有几个地方是 Mali 难以弥补的，比如系统缓存，针对少数几个配置的前端优化（Mali 需要兼顾 1-32 核）,以及确定的后端制程和优化。这里的每一项都可以提供 5%-10%的优化空间，积累起来也是不小的优势。

总之，GPU 的设计是一个不断细化的过程，选好大方向，把标准跑分明确，再注意新的趋势和需求，把模型和验证流程跑熟，剩下的就是不断打磨了。

### Reference

[1] https://zhuanlan.zhihu.com/p/32370404
