## What is LLM?

这是一个没有任何 AI 背景的程序员学习 LLM 的过程，其中记录会略显凌乱。

按照以往经验，接触到一个新的领域第一步是对该领域有个初步的，全面的了解，即知道该领域是做什么的，怎么做的，其间有什么关键的技术。该过程需要输出一个框架图。

进一步的，结合项目要求，深入理解某项技术，如 transformer 等。

# 学习资料

- 李宏毅教授的课程：https://www.bilibili.com/video/BV1TD4y137mP?p=2&vd_source=0d9f601780c903f80eac0aec6c712b83；
 - 课件：https://github.com/Fafa-DL/Lhy_Machine_Learning/tree/main?tab=readme-ov-file
- [Natural_Language_Processing_with_Transformers/chapter1.md at main · hellotransformers/Natural_Langua](https://github.com/hellotransformers/Natural_Language_Processing_with_Transformers/blob/main/chapter1.md)
- [ChatGPT 原理与应用开发](https://github.com/datawhalechina/hugging-llm/blob/main/docs/chapter0/前言.md)
- [huggingFace](https://huggingface.co/learn/nlp-course/zh-CN/chapter1/4)

# 基本概念

- 大模型（如 chatGPT）是怎样做到理解人类语言，并作出回答的？
 - 大模型做的是**做文字接龙的工作**，根据输入和已经输出的文字（token）经由一个复杂的函数输出下一个文字（token），所以它的输出是一个个往外蹦的。我们来分析一下这个函数是怎么实现的。

 - 找出这个函数的三步：

 - 设定范围，即确定候选函数的集合(model)；用到的技术有 Deep Learning(CNN, Transformer ...), Decision Tree, etc；
 - 设定目标：确定评价一个函数好坏的标准(loss)；Supervised Learning, Semi-supervised Learning, RL, etc；
 - 达成目标：找出最好的函数(optimization)；Gradient Descent(Adam, AdamW ...), Genetic Algorithm, etc；
 - CNN, RNN, transformer 就是生成这个函数的模型。

 - Data preparation 要怎样做？
 - 预训练 -> 督导式学习 -> 强化学习
- 大模型的参数是什么意思？
 - 正经的描述指的是模型内部用于存储知识和学习能力的数值。**这些参数可以被看作是模型的“记忆细胞”，它们决定了模型如何处理输入的数据、如何做出预测和生成文本等**。通俗来讲，**可以把GPT-3看作是一间超级大办公室的助理，里面有1750亿个抽屉（参数），每个抽屉里都放着一些特定的信息，包括单词、短语、语法规则、断句原则等**。

 - 当你向ChatGPT提问时，例如，帮我生成一个用于社交平台的鞋子营销文案。GPT-3这个助理就会去装有营销、文案、鞋子等抽屉中去提取信息，然后按照你的文本要求进行排列组合重新生成。

 - 在预训练过的程中，**GPT-3会像人类一样阅读大量的文本来学习各种语言和叙述结构**。每当它读到新信息或尝试生成新的文本方法时，都会打开这些抽屉查看里面的信息，并尝试找出最好的信息组合来回答问题或生成连贯的文本。当GPT-3在某些任务上表现得不够好时，会根据需要调整抽屉里的信息（更新参数），以便下次能做得更好。

 - 所以，每个参数都是模型在特定任务上的一个小决策点。更大的参数意味着模型可以有更多的决策能力和更细致的控制力，同时可以捕捉到语言中更复杂的模式和细节。
- 机器学习：帮助人类写出那个人类写不出来的函数。
 - 写出一个模型（函数集，function set）；
 - 设定衡量函数好坏的标准（loss）；
 - 找到 loss 值最好的那个函数；
- 深度学习：
- 有监督学习：人类给数据打标签，让机器学习；
- 无监督学习：
- 强化学习：
- 神经网络的权重和偏置：
- softmax：
- [CNN 和 RNN](https://blog.csdn.net/weixin_40551464/article/details/135472392)
 - CNN（convolutional neural network，卷积神经网络）和RNN（recurrent neural network，循环神经网络）是深度学习中的两种主要神经网络类型，它们在结构、功能和应用领域上有显著的区别：

 - **CNN**主要用于处理具有网格状拓扑结构的数据，如图像（2D网格）和视频（3D网格）。它通过卷积层来提取局部特征，这些卷积层可以捕捉空间上的相关性，如形状、纹理等。CNN通常还包括池化层（用于降低特征维度和增加网络深度）和全连接层；
 - **RNN**专门用于处理序列数据，如文本、时间序列数据等。它的特点是在时间步之间有循环连接，这意味着当前时间步的输出依赖于前一时间步的输出。这种结构使RNN能够记忆之前的信息，并在当前上下文中使用这些信息。
- [Transformer 和 RNN](https://blog.csdn.net/douyu0814/article/details/135365969)
 - Transformer和RNN是两种不同的序列建模模型。**Transformer通过自注意力机制和位置编码来捕捉序列中的依赖关系**，具有较好的并行性和处理长距离依赖的能力。而**RNN通过隐藏状态的传递来建模序列**，适用于短序列和对顺序信息较为敏感的任务。
- RAG(Retrieval Augmented Generation，检索增强生成)
 - RAG 的核心思想是**让语言模型在生成回答或文本时能够动态地从外部知识库中检索相关信息**。这种方法能够提高模型生成内容的准确性、可靠性和透明度，同时减少“幻觉”（即模型生成看似合理但实际上错误的信息）。

 - 构建一个 RAG 系统通常包括以下三个主要组成部分：

 - 语言模型：这是一个预先训练好的模型（可以在 hugging face 上下载一个开源大模型），能够根据给定的上下文生成文本；
 - 外部知识库：知识库中的信息以向量形式存储，便于快速检索和匹配；
 - 检索机制：这个组件负责在语言模型生成回答时检索相关的信息片段。通常使用某种形式的嵌入技术，将语言模型的输入和知识库中的条目进行比较，找出最相关的部分；
 - 以下是构建RAG系统的一般步骤：

 - 选择或训练语言模型：选择一个适合任务需求的预训练语言模型。
 - 构建知识库：根据需要处理的信息类型构建相应的知识库，并将知识库中的信息转换为适合快速检索的格式（如向量）。
 - 设计检索机制：实现一个检索组件，能够根据语言模型的输入查询知识库，并返回最相关的信息。
 - 整合与训练：将检索组件和语言模型整合，进行端到端的训练或微调，以优化整个系统的性能。
 - 在实际操作中，可以使用如CLIP（Contrastive Language-Image Pre-training）等多模态模型来增强RAG系统处理多种类型数据的能力。
- [Fine tunning](https://www.turing.com/resources/finetuning-large-language-models)
 - Primary fine-tuning approaches
 - Feature extraction (repurposing): The final layers of the model are then trained on the task-specific data while the rest of the model remains frozen;
 - Full fine-tuning: involves training the entire model on the task-specific data;
 - Prominent fine-tuning methods
 - Supervised fine-tuning: In this method, the model is trained on a task-specific labeled dataset, where each input data point is associated with a correct answer or label. The model learns to adjust its parameters to predict these labels as accurately as possible;
 - Reinforcement learning from human feedback (RLHF): An innovative approach that involves training language models through interactions with human feedback.
- [Chain of thought](https://www.cnblogs.com/bonelee/p/17406692.html)（COT，思维链）
 - ***Let's think step by step***

 - 语言模型很难将所有的语义直接转化为一个方程，因为这是一个更加复杂的思考过程，但可以通过中间步骤来更好地推理问题的每个部分。

 - 思维链提示就是把一个多步骤推理问题分解成很多个中间步骤，分配给更多的计算量，生成更多的 token，再把这些答案拼接在一起进行求解。

# Input -> output

大模型，其基础是语言模型，要完成理解 input 到输出 output，需要经过如下步骤：

## Token

**如何将自然语言文本表示成计算机能识别的数字**。对于一段文本来说，首先要做的就是把它变成一个一个Token。你可以将其理解为一小块，可以是一个字，也可以是两个字的词，或三个字的词。也就是说，给定一个句子时，我们有多种获取不同Token的方式，可以分词，也可以分字。英文现在都使用子词，比如单词annoyingly，会被拆成如下两个子词。

```C
["annoying", "##ly"]
```

## Embedding

一个Token表示成一定数量个小数（一般可以是任意多个，专业称呼为词向量的维度，根据所用的模型和设定的参数来确定），一般数字越多，模型越大，表示能力越强。如下面代码示例所示，每一行的若干个（词向量维度）小数就表示该位置的Token，词向量维度常见的值有：200、300、768、1536 等等。

```C
爱 [0.61048, 0.46032, 0.7194, 0.85409, 0.67275, 0.31967, 0.89993, ...]
不 [0.19444, 0.14302, 0.71669, 0.03338, 0.34856, 0.6991, 0.49111, ...]
对 [0.24061, 0.21482, 0.53269, 0.97885, 0.51619, 0.07808, 0.9278, ...]
古琴 [0.21798, 0.62035, 0.89935, 0.93283, 0.24022, 0.91339, 0.6569, ...]
你 [0.392, 0.13321, 0.00597, 0.74754, 0.45524, 0.23674, 0.7825, ...]
完 [0.26588, 0.1003, 0.40055, 0.09484, 0.20121, 0.32476, 0.48591, ...]
我 [0.07928, 0.37101, 0.94462, 0.87359, 0.55773, 0.13289, 0.22909, ...]
... ......................................................................
```

将任意文本（或其他非文本符号）表示成稠密向量的方法，可以统称为Embedding技术，它可以说是自然语言处理领域（其实也包括其他如图像、语音、推荐等领域）最基础的技术，后面的深度学习模型都是基于此。我们甚至可以稍微夸张点说，深度学习的发展就是Embedding表示技术的不断发展。

## 语言模型

语言模型（language model，LM），简单来说就是利用自然语言构建的模型。该模型使用 token 和 embedding 来表示自然语言，将 embedding 输入到模型之后，模型会计算出词表中每个 token 的概率，概率大的那个 token 就会被输出。举个例子，

给一个d维的向量（某个给定 Token），要输出一个长度为N的向量，N是词表大小，**N中每一个值都是一个概率值，表示下一个 Token 的概率**，加起来为1。按照贪婪匹配，**下个 Token 就是概率最大的那个**。写成简单的计算表达式如下。

```Plain
# d维，加起来和1没关系，大小是1×d，表示给定TokenX = [0.001, 0.002, 0.0052, ..., 0.0341]
# N个，加起来=1，大小是1×N，表示下个Token是每个Token的概率Y = [0.1, 0.5, ..., 0.005, 0.3]
# W是模型参数，也可以叫模型W·X = Y # W自然可以是 d×N 维的矩阵
```

上面的`W`就是模型的参数（参数越大，训练资料越丰富，计算出来的下一个 token 的概率就越准确），其实`X`也可以看作是参数自动学习到。因为我们知道了输入和输出的大小，所以中间其实可以经过任意随意的计算，也就是说这个`W`可以包含很多运算。总之就是各种张量（三维以上数组）运算，只要保证最后的输出形式不变就行。**这中间的各种计算就意味着各种不同的模型**。

### 训练

所谓训练就是就是输入 token，计算下一个 token 的概率，因为我们知道接下来每个位置的Token是啥，那我们这里得到最大概率的那个Token如果正好是这个Token，说明预测对了，参数就不用怎么调整；反之，模型就会调整前面的参数。

训练完成后，这些参数就不变了，然后就可以用前面同样的步骤来预测了，也就是给定一个Token预测下一个Token。如果是贪心搜索，每次给定同样Token时，生成的就一样。其余的就和前面讲的接上了。随着深度学习的不断发展，出现了很多复杂的网络结构，而且模型的参数变得非常多，但上面介绍的逻辑和方法是一样的。

总结起来就是：

- 将给定句子或文本表征成 Embedding；
- 将 Embedding 传入一个神经网络，计算得到不同标签的概率分布；
- 将上一步得到的标签概率分布与真实的标签做比较，并将误差回传，修改神经网络的参数，即训练；
- 得到训练好的神经网络，即模型。

## Transformer

Transformer 是一种用于序列建模的新型神经网络架构，该架构在机器翻译任务上的表现优于循环神经网络（RNN），在翻译质量和训练成本方面都是如此。

它是一个**基于注意力机制的编码器-解码器（Encoder-Decoder）架构**。刚开始主要应用在NLP领域，后来横跨到语音和图像领域，并最终统一几乎所有模态（文本、图像、语音）的架构。它来自Google2017年发的一篇论文：“Attention Is All You Need”，其最重要的核心就是提出来的**自注意力（Self-Attention）机制**。简单来说，就是**在语言模型建模过程中，把注意力放在那些重要的Token上**。

在弄懂 transformer 原理之前，我们需要介绍一下如下技术：

- 编码器-解码器框架
- 注意机制
- 迁移学习

### 编码器-解码器框架

Transformer 简单来说就是先把输入映射到编码器（Encoder），左边负责编码，右边则负责解码。这里面不同的是，左边因为我们是知道数据的，所以建模时可以同时利用当前 Token 的历史（后面的）Token 和未来（前面的）Token；但解码时，因为是一个 Token 一个 Token 输出来的，所以只能根据历史 Token 以及编码器的 Token 表示进行建模，而不能利用未来的 Token。

### 注意机制

编码器这一侧每个 Token 都可以输出一个向量（参考前面的 embedding 表示方式）表示，而这些所有Token的输出向量可以处理后作为整句话的表示。也可以取每个 Token 向量的平均值，也可以求和、取最大值等，用处理之后的 embeding 来表示整句话。现在重点来了，看解码器的过程，仔细看，其实它**在生成每一个Token时都用到了编码器每一个Token的信息，以及它已经生成的Token的信息**。前面这种**关注编码器中每个Token信息的机制就是注意力（Attention）机制**。直观点解释，当生成单词”power“时，“力量”两个字会被赋予更多权重（注意力）（这个更多的权重是怎样赋予的？），其他也是类似。

### 迁移学习

一种被称为ULMFiT的有效迁移学习方法表明，在一个非常大的、多样化的语料库上训练长短期记忆（LSTM）网络，可以在很少的标记数据下产生最先进的文本分类器（这些先进的文本分类器包括 GPT, BERT 等）。这一学习方法包括如下几步：

- 预训练：最初的训练目标相当简单。 根据前面的词来预测下一个词。 这项任务被称为语言建模。 这种方法的优雅之处在于不需要标注数据，人们可以利用维基百科等来源的大量文本；
- 领域适应性：一旦语言模型在大规模的语料库上进行了预训练，下一步就是使其适应领域内的语料库（例如，从维基百科到电影评论的 IMDb 语料库）。 这个阶段仍然使用语言建模，但现在该模型必须预测目标语料库中的下一个词；
- 微调：在这一步骤中，语言模型通过目标任务的分类层进行微调；

### Transformer 应用实例

- 文本分类：
- 命名实体识别 （NER)：
- 问答系统：
- 摘要：
- 机器翻译：
- 文本生成：

### 使用Transformers的主要挑战

- 语言：英文是 NLP 研究的主要目标语言，也存在几个其它语言的模型，但是很难找到针对小语种或低资源语种的预训练模型；
- 数据可获取能力：大模型训练需要经过标注的数据集，可以使用迁移学习来大幅减少模型所需的标注训练数据量，但与人类执行任务所需的数据量相比，这仍然是一个很大的差距；
- 长文本问题：自注意力模型在段落较长的文本中效果非常好，但当我们转向较长的文本（如整个文档）时，它会变得非常昂贵；
- 不可解释性：与其他深度学习模式一样，Transformers 在很大程度上是不透明的。很难或不可能解释一个模型做出某种预测的“原因”；
- 偏见：Transformers 模型主要是在互联网的文本数据上进行预训练。 这就把数据中存在的所有偏见都印在了模型中。 确保这些既不是种族主义者、性别歧视者，也不是更糟糕的人，是一项具有挑战性的任务；

### Hugging face 实践

### Reference

- https://github.com/oobabooga/text-generation-webui?tab=readme-ov-file

- #####
