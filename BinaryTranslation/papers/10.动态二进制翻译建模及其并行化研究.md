## 动态二进制翻译建模及其并行化研究

### 1. 基于私有缓存的间接分支查找算法

基本块的是以分支指令结尾的，基本块之间的跳转关系也由分支指令确定。直接分支指令在编译的时候确定，间接分支指令在程序运行时确定。间接分支指令造成的跳转要进行上下文切换，返回到查找模块根据间接分支的目标地址查找下一个基本块进而翻译或执行，降低效率。但是间接分支的目标地址有良好的局部性。根据这一局部性通过私有缓存保存跳转地址，然后利用这些跳转历史信息来减少上下文切换次数从而提高翻译效率。

#### 1.1. 间接分支处理算法

（1）间接分支哈希表

在查找模块中使用一个**全局的哈希表**来查找分支指令的目标地址。这就是常用的方法，需要保存和恢复上下文，破坏了程序局部性。

（2）目标地址内联

在间接分支翻译的控制转移代码中**内嵌一些可能的间接分支目标**，在执行到该间接分支时，与保存的分支目标比较。如果相等，则预测成功，直接跳转到对应的翻译好的基本块代码执行；如果不相等，则正常查找、翻译、执行。但这种方法最大的问题是**没有好的映射算法**，需要在所有保存的分支中比较，降低效率。同时**没有好的预测算法**，即保存哪些分支指令。

#### 1.2. 间接分支目标地址的局部性

间接跳转分支的目标地址分布**具有较好的局部性**。可以利用这一特性设计间接分支的查找、预测算法。

![](https://github.com/UtopianFuture/UtopianFuture.github.io/blob/master/image/10.1.png?raw=true)  

#### 1.3. 带有私有缓存的目标地址查找算法

QEMU中的Translation Block是描述基本块的数据结构。作者在每个分支跳转指令所在的基本块的Translation Block中添加一个**私有缓存**，它存储的是**目标地址所在的**Translation Block的地址指针。这个私有缓存是一个哈希数组，索引是源程序的跳转目标地址（SPC）对哈希数组大小的取模。

前端在查找或翻译得到一个新的基本块后，如果上一个基本块是以间接分支结尾的，该算法总是把当前基本块放入上一个基本块的私有缓存中，**如果发生冲突，则直接替换**。

后端在执行到间接分支指令时，以SPC对私有缓存大小的取模作为索引获得对应的Translation Block地址指针。将SPC与Translation Block首条指令的指令地址比较，如果相等，则预测成功，直接跳转到对应块执行；如果不相等，预测失败，则返回到前端，正常查找、翻译、执行。

其中，私有缓存保存的预测数目会影响预测的成功率。经实验，私有缓存为**16**项时，预测率与空间大小达到比较好的平衡。

### 2. 多线程翻译模型优化

传统的DBT系统通常使用单线程模型运行，翻译模块和执行模块之间的数据流时严格串行的，难以利用丰富的多核资源。就像之前我想的可以将反汇编、翻译、执行等步骤通过多线程来实现，类似于流水线。多线程的DBT模型通常包括线程任务的划分方式，多线程预测算法，同步互斥算法，代码缓存管理等方面。

#### 2.1. 线程任务划分

合理的划分模式可以提高多线程模型的并行度和扩展性。线程任务划分可以分为三种模式：一对一模式、一对多模式、主从模式。

（1）一对一模式

​     一个线程负责反汇编、翻译，同时还要对未来可能甬道的基本块进行预测翻译， 一个线程负责执行。执行线程首先判断要执行的基本块是否已经被翻译，如果没有被翻译，则将对该基本块的翻译请求放入任务请求队列而自己进入等待队列。一旦该翻译块翻译完成，执行线程恢复执行状态。翻译线程在被唤醒后，首先检查是否有需要翻译的基本块，有则先执行翻译请求，因为翻译请求具有最高优先级；没有则预测翻译。

但两个线程并行度不高。当翻译线程响应执行线程的翻译请求时，预测翻译就不能同时进行。同时翻译线程在执行预测翻译时也不能及时响应执行线程的翻译请求。

（2）一对多模式

一个执行线程，多个翻译线程。当所有的翻译线程处于预测翻译状态，也不能及时响应执行线程的翻译请求，可能会造成执行停顿。而且多个翻译线程管理复杂。

（3）主从模式

使用一个主线程负责翻译和执行，多个从线程进行预测翻译（本文使用的，换汤不换药）。

#### 2.2. 多线程预测翻译算法

此算法的目的时尽可能准确及时的预测出未来要执行的基本块。需要考虑如何管理多个线程间的同步互斥，负载均衡和可扩展性。整个算法分为两部分：目标地址预测算法和线程管理算法。

（1） 目标地址预测算法

当前的预测算法有：

- 基于队列预测算法。
- 跳转树预测算法。
- 基于栈结构预测算法。根据局部性原理，从较新基本块挖掘出的预测翻译请求未来可能被执行。故把当前正在翻译的基本块的可能跳转目标指令作为预测翻译任务压入翻译请求栈。然后预测翻译线程根据后进先出的原则进行预测翻译，如果发现新的潜在目标块，产生新的预测翻译任务，压入翻译请求栈。该算法能够保证正在执行的基本块周围的基本块能够快速翻译。

（2）线程管理算法

#### 2.3. 分布式代码缓存管理与优化

在多线程动态二进制翻译模型中，多个线程都需要访问代码缓存，故代码缓存成了临界资源。需要算法管理代码缓存，减少线程间的相互竞争。

文章提出的算法思想如下：以主线程的代码缓存为主缓存，每次主线程查找总是先查询自己的代码缓存。一旦主线程在其他从线程的代码缓存中找到目标基本块时，总是使用memcpy函数将目标基本块拷贝到主线程的代码缓存，然后执行。因为频繁的内存拷贝，memcpy会影响系统效率，故loongson CPU对memcpy进行专门的优化。

### 3. 线程级并行系统级模拟器原子指令模拟方法

在多核体系结构中，多个计算核心会共享内存，故会造成读写竞争。为了正确同步各个CPU核心之间的操作，现代多核处理器提供了原子指令保证指令对共享内存操作的原子性。

![](https://github.com/UtopianFuture/UtopianFuture.github.io/blob/master/image/10.2.png?raw=true)

传统的DBT模拟方式是单线程的，即用时间片轮转的方式依次模拟各个计算核心，所有的指令都是顺序执行的且不会被其他模拟的处理器干扰，导致即使存在共享内存竞争和冲突的多线程程序在该种模拟方式下也能正确的执行。这就是传统模拟方式不能正确模拟多核CPU的重要方面。

#### 3.1. 原子指令模拟方法

（1）基于互斥锁的原子指令模拟

通过对操作临界资源的指令加行加解锁操作，能够保证临界资源能够被互斥的访问，使用互斥锁模拟原子指令是最直观的方法。但加锁的原子指令和非原子指令同时访问临界资源还是会造成冲突。若对所有访问临界资源的指令都加锁则开销过大。

（2）同步信号的原子指令模拟

当一个处理器需要访问临界资源时，发送请求等待（WAIR_REQ）信号来使其他处理器都中断，进入阻塞模式。代价太大，牺牲同步性，不合适。

（3）非阻塞同步算法的原子指令模拟

非阻塞同步机制是指一个进程停止，挂起或阻塞不会导致其他进程也停止，挂起或阻塞。

（4）内核支持的原子指令模拟

### 4. 思考

4.1. 间接分支的优化算法挺好的：为每条间接分支保存几条预测的目标指令，减少上下文切换次数。实现起来并不复杂。

4.2. 将反汇编、翻译和执行分成不同的线程实现，类似于流水线，然后还加上预测翻译线程。能充分利用多核CPU优势，但多线程编程是个问题。

4.3. 根据程序的局部性采用的基本块预测翻译能够提前翻译当前执行基本块的周边基本块。

4.4. 原子指令模拟对我来说还太难了，暂时放弃。