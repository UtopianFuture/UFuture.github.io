## KVM原理简介

### 1. 硬件虚拟化技术

### 1.1. CPU虚拟化

Intel在处理器级别提供了对虚拟化技术的支持，被称为VMX（virtual-machine extensions）。有两种VMX操作模式：**VMX根操作（root operation）**与**VMX非根操作（non-root operation）**。作为虚拟机监控器中的KVM就是运行在根操作模式下，而虚拟机客户机的整个软件栈（包括操作系统和应用程序）则运行在非根操作模式下。进入VMX非根操作模式被称为“**VM Entry**”；从非根操作模式退出，被称为“**VM Exit**”。

x86架构本身是不能进行全虚拟化的，因为其存在虚拟化漏洞，而VMX的非根操作模式就是为了适应虚拟化而专门做了一定的修改；在客户机中执行的一些**敏感指令**或者一些异常会触发“VM Exit”退到虚拟机监控器中，从而运行在VMX根模式。正是这样的限制，让虚拟机监控器保持了对处理器资源的控制（这很重要）。

### 1.2 内存虚拟化

内存虚拟化的目的是给虚拟客户机操作系统提供一个从0地址开始的连续物理内存空间，同时在多个客户机之间实现隔离和调度。在虚拟化环境中，内存地址的访问会涉及如下四个概念：

1）客户机虚拟地址，GVA（Guest Virtual Address）

2）客户机物理地址，GPA（Guest Physical Address）

3）宿主机虚拟地址，HVA（Host Virtual Address）

4）宿主机物理地址，HPA（Host Physical Address）

![](https://github.com/UtopianFuture/UtopianFuture.github.io/blob/master/image/kvm.1.png?raw=true)  
内存虚拟化就是要将GVA转化为最终能够访问的HPA。在没有硬件提供的内存虚拟化之前，系统通过**影子页表（**Shadow Page Table)完成这一转化。内存的访问和更新通常是非常频繁的，要维护影子页表中对应关系会非常复杂，开销也较大。同时需要为每一个客户机都维护一份影子页表，当客户机数量较多时，其影子页表占用的内存较大也会是一个问题。

Intel CPU在硬件设计上就引入了**EPT**（Extended Page Tables，扩展页表），从而将GVA到HPA的转换通过硬件来实现。这个转换分为两步，如图所示。首先，通过客户机CR3寄存器将GVA转化为GPA，然后通过查询EPT来实现GPA到HPA的转化。

EPT的控制权在虚拟机监控器中，只有当CPU工作在非根模式时才参与内存地址的转换。使用EPT后，客户机在读写CR3和执行INVLPG指令时不会导致VM Exit，而且客户页表结构自身导致的页故障也不会导致VM Exit。所以通过引入硬件上EPT的支持，简化了内存虚拟化的实现复杂度，同时也提高了内存地址转换的效率。

![](https://github.com/UtopianFuture/UtopianFuture.github.io/blob/master/image/kvm.2.png?raw=true)  


### 1.3 设备虚拟化

在虚拟化的架构下，虚拟机监控器必须支持来自客户机的I/O请求。通常情况下有以下4种I/O虚拟化方式。

（1）设备模拟：在虚拟机监控器中模拟一个传统的I/O设备的特性，比如在QEMU中模拟一个Intel的千兆网卡或者一个IDE硬盘驱动器，在客户机中就暴露为对应的硬件设备。客户机中的I/O请求都由虚拟机监控器捕获并模拟执行后返回给客户机。

（2）前后端驱动接口：在虚拟机监控器与客户机之间定义一种全新的适合于虚拟化环境的交互接口，比如常见的virtio协议就是在客户机中暴露为virtio-net、virtio-blk等网络和磁盘设备，在QEMU中实现相应的virtio后端驱动。

（3）设备直接分配：将一个物理设备，如一个网卡或硬盘驱动器直接分配给客户机使用，这种情况下I/O请求的链路中很少需要或基本不需要虚拟机监控器的参与，所以性能很好。

（4）设备共享分配：其实是设备直接分配方式的一个扩展。在这种模式下，一个（具有特定特性的）物理设备可以支持多个虚拟机功能接口，可以将虚拟功能接口独立地分配给不同的客户机使用。如SR-IOV就是这种方式的一个标准协议。

![](https://github.com/UtopianFuture/UtopianFuture.github.io/blob/master/image/kvm.3.png?raw=true)  


设备直接分配在Intel平台上就是VT-d（Virtualization Technology For Directed I/O）特性，一般在系统BIOS中可以看到相关的参数设置。Intel VT-d为虚拟机监控器提供了几个重要的能力：I/O设备分配、DMA重定向、中断重定向、中断投递等。

### 2. KVM架构概述

 KVM就是在硬件辅助虚拟化技术之上构建起来的**虚拟机监控器**。当然，并非要所有这些硬件虚拟化都支持才能运行KVM虚拟化，KVM对硬件**最低的依赖是**CPU的硬件虚拟化支持（也就是说，KVM可以运行在所有支持CPU虚拟化的机器上），比如：Intel的VT技术和AMD的AMD-V技术，而其他的内存和I/O的硬件虚拟化支持，会让整个KVM虚拟化下的性能得到更多的提升。

KVM虚拟化的核心主要由以下两个模块组成：

（1）**KVM内核模块**，它属于标准Linux内核的一部分，是一个专门提供虚拟化功能的模块，主要负责CPU和内存的虚拟化，包括：客户机的创建、虚拟内存的分配、CPU执行模式的切换、vCPU寄存器的访问、vCPU的执行。

（2）**QEMU用户态工具**，它是一个普通的Linux进程，为客户机提供设备模拟的功能，包括模拟BIOS、PCI/PCIE总线、磁盘、网卡、显卡、声卡、键盘、鼠标等。同时它通过ioctl系统调用与内核态的KVM模块进行交互。

KVM是在硬件虚拟化支持下的**完全虚拟化技术**，所以它能支持在**相应硬件**上能运行的几乎所有的操作系统，x86下如：Linux、Windows、FreeBSD、MacOS等。KVM的基础架构如图所示。在KVM虚拟化架构下，**每个客户机就是一个QEMU进程**，在一个宿主机上有多少个虚拟机就会有多少QEMU进程；客户机中的每一个虚拟CPU对应QEMU进程中的一个执行线程；一个宿主机中只有一个KVM内核模块，所有客户机都与这个内核模块进行交互。

![](https://github.com/UtopianFuture/UtopianFuture.github.io/blob/master/image/kvm.4.png?raw=true)  

### 3. KVM内核模块

KVM内核模块是标准Linux内核的一部分，由于KVM的存在让Linux本身就变成了一个**Hypervisor**，可以原生地支持虚拟化功能。目前，**KVM支持多种处理器平台**，它支持最常见的以Intel和AMD为代表的x86和x86_64平台，也支持PowerPC、S/390、ARM等非x86架构的平台。

KVM模块的任务是**打开并初始化系统硬件**以支持虚拟机的运行。以KVM在Intel公司的CPU上运行为例，在被内核加载的时候，KVM模块会先初始化内部的数据结构；做好准备之后，KVM模块检测系统当前的CPU，然后打开CPU控制寄存器CR4中的虚拟化模式开关，并通过执行VMXON指令将宿主操作系统（包括KVM模块本身）置于CPU执行模式的虚拟化模式中的根模式；最后，KVM模块创建特殊设备文件/dev/kvm并等待来自用户空间的命令。接下来，虚拟机的创建和运行将是一个用户空间的应用程序（QEMU）和KVM模块相互配合的过程。

### 4. QEMU用户态设备模拟

与KVM不同，QEMU最初实现的虚拟机是一个纯软件的实现，通过二进制翻译来实现虚拟化客户机中的CPU指令模拟，所以性能比较低。但是，其优点是跨平台，跨ISA。

QEMU的代码中有完整的虚拟机实现，包括处理器虚拟化、内存虚拟化，以及KVM也会用到的**虚拟设备模拟**（比如网卡、显卡、存储控制器和硬盘等）。除了二进制翻译的方式，QEMU也能与基于硬件虚拟化的Xen、KVM结合，为它们提供客户机的设备模拟。通过与KVM的密切结合，让虚拟化的性能提升得非常高，在真实的企业级虚拟化场景中发挥重要作用，所以我们通常提及KVM虚拟化时就会说“QEMU/KVM”这样的软件栈。

虚拟机运行期间，QEMU会通过KVM模块提供的系统调用进入内核，由KVM模块负责将虚拟机置于处理器的特殊模式下运行。遇到虚拟机进行I/O操作时，KVM模块会从上次的系统调用出口处返回QEMU，由QEMU来负责解析和模拟这些设备。

总之，QEMU既是一个功能完整的虚拟机监控器，也在QEMU/KVM的软件栈中承担设备模拟的工作。