## Network

这一篇谈下网络处理器。

曾几何时，网络处理器是高性能的代名词。为数众多的核心，强大的转发能力，定制的总线拓扑，专用的的指令和微结构，许多优秀设计思想沿用至今。Tilera，Freescale，Netlogic，Cavium，Marvell 各显神通。但是到了 2018 年，这些公司却大多被收购，新闻上也不见了他们的身影，倒是交换芯片时不时冒出一些新秀。

随着移动互联网的兴起，网络设备总量实际上是在增加的。那为什么网络芯片反而没声音了呢？究其原因有几点：

第一， 电信行业利润率持续减少。10 年之前，Freescale 的 65 纳米双核处理器，可以卖到 40 美金。如今同样性能的 28 纳米芯片，10 美金都卖不到。而网络处理器的量并不大，低端有百万片已经挺不错了，高端更是可能只有几万片。这使得工程费用无法均摊到可以忽略的地步，和手机的动辄几亿片没法比。而单价上，和服务器上千美金的售价也没法比。这就使得网络处理器陷入一个尴尬境地。

第二， IP 模式的兴起。利润的减少导致芯片公司难以维持专用处理器设计团队。电信行业的开发周期普遍很长，而网络处理器从立项到上量，至少需要 5 年时间，供货周期更是长达 15 年。一个成熟的 CPU 团队，需要 80 人左右，这还不算做指令集和基础软件的，全部加上超过 100 人。这样的团队开销每年应该在 2000 万刀。2000 万刀虽然不算一个很大的数字，可如果芯片毛利是 40%，净利可能只有 10%，也就是要做到 2 亿美金的销售额才能维持。这还只是 CPU 团队，其余的支出都没包含。通常网络芯片公司销售额也就是 5 亿美金，在利润不够维持处理器研发团队后，只能使用通用的 IP 核，或者对原有 CPU 设计不做改动。另外，还要担心其他高利润公司来挖人。以 Freescale 为例，在使用 PowerPC 的最后几年，改了缓存设计后，甚至连验证的工程师都找不到了。

第三， 指令集的集中化。在五年前，网络处理器还是 MIPS 和 PowerPC 的世界，两个阵营后各有几家在支持。当时出现了一个很有趣的现象，做网络设备的 OEM 公司会主动告诉芯片公司，以后的世界，是 x86 和 ARM 指令集的世界，为了软件的统一，请改成 ARM 指令集。在当时，ARM 指令集在网络界似乎只有 Marvell 一家在使用。而五年之后的现在，PowerPC 和 MIPS 基本退出舞台，只有一些老产品和家用网关还有它们的身影。

在网络处理器领域，ARM 并无技术和生态优势：技术上，MIPS 大都是单核多线程的，支持自定义的指令集，对一些特定的网络包处理添加新指令。而 PowerPC 很早就使用了 Cache
Stashing 等专用优化技术，ARM 直到近两年才支持。即使使用了 ARM 指令集，Cavium 和 Netlogic 还是自己设计微结构。生态上，电信产品的应用层软件，都是 OEM 自定义的。底层操作系统，Linux 和 VxWorks，对 MIPS 和 PowerPC 的支持并不亚于 ARM。而中间层的软件，虚拟机和数据转发，NFV 等，网络芯片公司之前也并不是依靠 ARM 来维护的。赢得网络处理器市场还真是 ARM 的意外之喜。

第四， OEM 公司自研芯片的崛起。思科，华为，中兴等公司，早就开始自己做技术含量高的高端芯片，或者量大的低端芯片。对于高端产品，只有使用自研芯片才能抢到第一个上市，对于低端产品，就主要是成本考虑了。

让我们看看网络处理器到底能应用在哪些场景：

![img](https://pic4.zhimg.com/80/v2-402fd6623fd9f6629de7c14de6367a5b_1440w.webp)

首先，是终端设备（手机/基站，wifi，光纤/Cable/ADSL 等），连接到接入网，然后再汇聚到核心网。就节点吞吐量而言，大致可以分为几类：

第一， 在终端，吞吐量在 1Gbps 级别，可以是手机，可以是光纤，可以是有线电视，也可以是 Wifi，此时需要小型家庭路由器，无需专用网络加速器，对接口有需求，比如 ATM，光纤，以太网或者基带芯片。

第二， 在基站或者边缘数据中心，吞吐量在 1Tbps 级别，此时需要边缘路由器，基站还需要 FPGA 或者 DSP 做信号处理。

第三， 在电信端，吞吐量远大于 1Tbps，此时需要核心路由器，由专用芯片做转发。

以上场景中，低端的只需要 1-2 核跑所有的数据面和控制面程序就行；对于中端应用，控制面需要单核能力强的处理器，数据面需要能效高，核数众多的处理器和网络加速器；高端路由器需要专用 asic 或者定制的超小核进行数据面处理，同时使用单核能力强的处理器做控制面。

这其中比较有争议的是中端的数据面处理。对中端路由器来说，业务层的协议处理，毫无疑问的该使用通用处理器；而一些计算密集的功能，比如模板检测，加解密，压缩，交换等，还是用 asic 做成模块比较合适；对于队列管理，保序，优先级管理，资源管理等操作，就见仁见智了。从能效和芯片面积上来说，通用处理器做这些事肯定不是最优的，但是它设计容错性高，扩展性好，产品设计周期短，有着专用加速器无法取代的强项。硬件加速器本身会有 bug，需要多次流片才能成熟，这非常不利于新产品的推广，而通用处理器方案就能大大减少这个潜在风险。专用网络加速器还有个很大的缺点，就是很难理解和使用，OEM 如果要充分发挥其性能，一定需要芯片公司配合，把现有代码移植，这其中的推广时间可能需要几年。不过有时候芯片公司也乐见其成，因为这样一来，至少能把这一产品在某种程度上和自己捆绑一段时间。

再进一步，有些任务，如路由表的查找，适合用单核性能高的处理器来实现；有些任务，如包的业务代码，天然适合多线程，用多个小核处理更好。这就把通用处理器部分也分化成两块，大核跑性能需求强的代码，小核跑能效比需求强的代码。大小核之间甚至可以运行同一个 SMP 操作系统，实现代码和数据硬件一致性。事实上，有些网络处理器的加速器，正是用几十上百个小 CPU 堆起来的。

在网络芯片上，有两个重要的基本指标，那就是接口线速和转发能力。前者代表了接口能做到的数据吞吐量上限，后者代表了处理网络包的能力。

![img](https://pic4.zhimg.com/80/v2-7a713eac469143912bb891867a66a9a7_1440w.webp)

如上图，以太网包最小的长度是 64 字节，这只是数字部分的长度。考虑到 PHY 层的话，还有 8 字节的 Preamble 和开始标志，外加帧间分隔，总共 84 字节。

对于 1Gbps 的以太网，能够发送的以太网包理论上限是 1.45Mpps(Million Packet Per Second)。相应的，处理器所能处理的包数只需要 1.45Mpps，超出这个数字的能力是无用的。线速一定时，以太网包越大，所需的处理能力越低，所以在衡量处理器能力的时候，需用最小包；而在衡量网络接口能力的时候，需要用最大包，来使得帧间浪费的字段达到最小比例。

下面是 Freescale 的 LX2160 网络处理器的指标：

![img](https://pic1.zhimg.com/80/v2-361b39256cee50641dfdfc82ce2fcfd4_1440w.webp)

我们可以看到，共有 24 个 28Gbps 的 Serdes 口，对应的，2 个 100G 以太网口需要 8 个 Serdes 口，而 14 个 25G 以太网口需要 14 个 Serdes 口，共 22x25=550Gbps 的吞吐率，最大需要 800Mpps 的处理能力。分散到 16 个核，每个核需要 50Mpps 的转发能力，对应到 2.2Ghz 的处理器，也就是 44 个时钟周期处理一个包。事实上，这是不现实的。通常，一个包经过 Linux 协议栈，即使什么都不做，也需要 1500 个时钟周期，就算使用专业转发软件，至少也需要 80 个左右。所以，最小 64 字节包只是一个衡量手段，跑业务的时候肯定需要尽量使用大包。

所以，在定义网络处理器的规格时，需要在线速和处理能力上做平衡。

![img](https://pic4.zhimg.com/80/v2-b8be712bdfa7d7d55a80346b4af94647_1440w.webp)

上图是 Freescale 的 LX2160 网络处理器内部模块图。按照之前的线速和处理能力来看，可以分为三块：

第一， 处理器部分。这里是 16 个 A72 核，每两个放在一个组里，分配 1MB 二级缓存。在手机上，通常是四个核放在一组，这里为什么是两个？因为网络处理主要是针对包的各种处理，更偏重于 IO 和访存的能力，而不仅仅是计算。在处理包时，不光是网络包的数据需要读进处理器，还需要读入各种参数，来完成对包的处理。这些参数针对每个包会有所不同，缓存可能是放不下的，需要从内存读取。这就造成了需要相对较大的带宽。在架构篇，我们定量计算过，当只有一个处理器在做数据读取的时候（假设都不在缓存），瓶颈在核的对外接口 BIU，而四个核同时读取，瓶颈在 ACE 接口。这就是每组内只放两个核而不是四个的原因。而处理器总的核数，很大程度上决定了网络处理器的包处理能力。

第二， 接口部分。网络处理器通常会把控制器（以太网，SATA，PCIe 等）和高速串行总线口 Serdes 复用，做成可配置的形式，提高控制器利用率，节省面积。这里使用了 24 对差分线，每条通道支持 25Gbps。高速串行总线设计也是一个难点，经常会有各种串扰，噪声，抖动等问题，搞不好就得重新流片。（推荐阅读：[又见山人：为什么串口比并口快？](https://www.zhihu.com/question/27815296/answer/38699109)）

第三， 网络加速器，上图中的 WRIOP 和 QB-Man 属于网络加速器。这是网络处理器精华的设计，其基本功能并不复杂，如下图：

![img](https://pic3.zhimg.com/80/v2-7eb96a98393a61cde280fc5d181c403e_1440w.webp)

一个包从以太网进来，如果没有以太网加速器，那会被以太网控制器用 DMA 的方式放到内存，然后就丢给 CPU 了。而硬件加速器的作用，主要是对包的指定字段进行分析，打时间戳，然后分类并分发到特定的队列。而这些队列，在经过重映射后，会提供给 CPU 来处理。这其中还牵涉到队列关系，优先级，限速，排序，内存管理等功能。加解密模块可以看作独立的模块。就我所见，网络加速器需要和所有处理器之和相近的面积。

我们来看看一个包到底在网络处理器会经历哪些步骤：

首先，CPU 调用以太网驱动，设置好入口(Ingress)描述符。以太网控制器根据描 Ingress 述符将 Serdes 传来的包，DMA 到指定内存地址，然后设置标志位，让 CPU 通过中断或者轮询的方式接收通知。然后 CPU 把包头或者负载读取到缓存，做一定操作，然后写回到出口(Egress)描述符所指定的内存，并再次设置标志位，通知以太网控制器发包。Ingress 和 Egress 包所存放的地址是不同的。以太网控制器根据 Egress 描述符将包发送到 Serdes，并更新描述符状态。

![img](https://pic1.zhimg.com/80/v2-edca9203f2ee96af0e500c969b18fe30_1440w.webp)

将上述步骤实现到上图的系统中，并拆解成总线传输，是这样的（假定代码及其数据在一二级缓存中）：

\1. CPU 写若干次以太网控制器(10-40GbE)的寄存器（Non-cacheable）

\2. CPU 读若干次以太网控制器(10-40GbE)的状态寄存器（Non-cacheable）

\3. 以太网控制器(10-40GbE)进行若干次 DMA，将数据从 PHY 搬到内存

\4. CPU 从内存读入所需包头数据，做相应修改

\5. CPU 写入内存所需数据

\6. CPU 写若干次以太网控制器(10-40GbE)的寄存器（Non-cacheable）

\7. 以太网控制器(10-40GbE)进行若干次 DMA，将数据从内存搬到 PHY，并更新内部寄存器

\8. CPU 读若干次以太网控制器(10-40GbE)的状态寄存器（Non-cacheable）

以上 8 步所需时间通常为：

\1. 1x 以太网寄存器访问延迟 。假定为 20ns。

\2. 1x 以太网寄存器访问延迟。假定为 20ns。

\3. 以太网控制器写入 DDR 的延迟。假设是最小包，64 字节的话，延迟 x1。如果是大包，可能存在并发。但是和第一步中 CPU 对寄存器的写入存在依赖关系，所以不会小于延迟 x1。假定为 100ns。

\4. CPU 从 DDR 读入数据的延迟。如果只需包头，那就是 64 字节，通常和缓存行同样大小，延迟 x1。假定为 100ns。

\5. CPU 写入 DDR 延迟 x1。假定为 100ns。

\6. 1x 以太网寄存器访问延迟 。假定为 20ns。

\7. 以太网从 DDR 把数据搬到 PHY，延迟至少 x1。假定为 100ns。

\8. 1x 以太网寄存器访问延迟。假定为 20ns。

2 和 3，7 和 8 是可以同时进行的，取最大值。其余的都有依赖关系，可以看做流水线上的一拍。其中 4，5 和 6 都在 CPU 上先后进行，针对同一包的处理，它们无法并行（同一 CPU 上的不同包之间这几步是可以并行的）。当执行到第 6 步时， CPU 只需要将所有非 cacheable 的写操作，按照次序发出请求即可，无需等待写回应就可以去干别的指令。这样，可以把最后一步的 所流水线中最大的延迟缩小到 100+20+100 共 220ns 以下的某个值。当然，因为 ARM 处理器上 non-cacheable 的传输，在对内部很多都是强制排序的，无法并行，总线也不会提早发送提前回应(early response)，所以总延迟也就没法减少太多。

上面的计算意味着，CPU 交换一下源和目的地址，做一下简单的小包转发，流水后每个包所需平均延迟是 220ns。同时，增大带宽和 CPU 数量并不会减少这个平均延迟，提高 CPU 频率也不会有太大帮助，因为延迟主要耗费在总线和 DDDR 的访问上。提高总线和 DDR 速度倒是有助于缩短这个延迟，不过通常能提高的频率有限，不会有成倍提升。

那还有什么办法吗？一个最直接的办法就是增加 SRAM，把 SRAM 作为数据交换场所，而不需要到内存。为了更有效的利用 SRAM，可以让以太网控制器只把包头放进去，负载还是放在放 DDR，描述符里面指明地址即可。这个方法的缺点也很明显：SRAM 需要单独的地址空间，需要特别的驱动来支持。不做网络转发的时候，如果是跑通用的应用程序，不容易被利用，造成面积浪费。

第二个方法是增加系统缓存，所有对 DDR 的访问都要经过它。它可以只对特殊的传输分配缓存行，而忽略其余访问。好处是不需要独立的地址空间，缺点是包头和负载必须要指定不同的缓存分配策略。不然负载太大，会占用不必要的缓存行。和 SRAM 一样，包头和负载必须放在不同的页表中，地址不连续。

在有些处理器中，会把前两个方法合成，对一块大面积的片上内存，同时放缓存控制器和 SRAM 控制器，两种模式可以互切。

第三个方法，就是在基础篇提到的 Cache stashing，是对第二种方法的改进。在这里，包头和负载都可以是 cacheable 的，包头被以太网控制器直接塞到某个 CPU 的私有缓存，而负载则放到内存或者系统级缓存。在地址上，由于缓存策略一致，它们可以是连续的，也可以分开。当 CPU 需要读取包头时，可以直接从私有缓存读取，延迟肯定短于 SRAM，系统缓存或者 DDR。再灵活一点，如果 CPU 需要更高层协议的包头，可以在以太网控制器增加 Cache sashing 的缓存行数。由于以太网包的包头一定排列在前面，所以可以将高层协议所需的信息也直接送入某个 CPU 一二级私有缓存。对于当中不会用到的部分负载，可以设成 non-cacheable 的，减少系统缓存的分配。这样的设计，需要把整个以太网包分成两段，每一段使用不同的地址空间和缓存策略，并使用类似 Scatter-Gather 的技术，分别做 DMA，增加效率。这些操作都可以在以太网控制器用硬件和驱动实现，无需改变处理器的通用性。

此外，有一个小的改进，就是增加缓存行对 CPU，虚拟机，线程 ID 或者某个网络加速器的敏感性，使得某些系统缓存行只为某个特定 ID 分配缓存行，可以提高一些效率。

以上解决了 CPU 读包头的问题，还有写的问题。如果包头是 Cacheable 的，不必等待写入 DDR 之后才收到写回应，所以时间可以远小于 100ns。至于写缓存本身所需的读取操作，可以通过无效化某缓存行来避免，这在基础篇有过介绍。这样，写延迟可以按照 10-20ns 计算。

以太网控制器看到 Egress 的寄存器状态改动之后，也不必等待数据写到 DDR，就可以直接读。由于存在 ACE/ACE-lite 的硬件一致性，数据无论是在私有缓存，系统缓存还是 DDR，都是有完整性保证的。这样一来，总共不到 50ns 的时间就可以了。同理，Ingress 通路上以太网控制器的写操作，也可以由于缓存硬件一致性操作而大大减少。

相对于第二种方法，除了延迟更短，最大的好处就是，在私有缓存中，只要被 cache stashing 进来的数据在不久的将来被用到，不用担心被别的 CPU 线程替换出去。Intel 做过一些研究，这时间是 200ns。针对不同的系统和应用，肯定会不一样，仅作参考。

让我们重新计算下此时的包处理延迟：

原来的第四步，CPU 读操作显然可以降到 10ns 之下。

访问寄存器的时间不变。

CPU 写延迟可以按照 10-20ns 计算。并且可以省略，因为处理器在写操作之后可以立刻返回，无需停顿。

总时间从 220 降到了 50ns 以下，四倍。

不过，支持 Cache Stashing 需要 AMBA5 的 CHI 协议，仅仅有 ACE 接口是不够的。CPU，总线和以太网控制器需要同时支持才能生效。如果采用 ARM 的 CPU 设计，需要最新的 A75/A55 才有可能。总线方面，A75/A55 自带的 ACP 端口可用于 20GB 以下吞吐量的 Cache Stashing，适合简单场景，而大型的就需要专用的 CMN600 总线以支持企业级应用了。还有一种可能，就是自己做 CPU 微架构和总线，就可以避开 CHI/ACE 协议的限制了。

Intel 把 cache stashing 称作 Direct Cache Access，并且打通了 PCIe 和以太网卡，只要全套采用 Intel 方案，并使用 DPDK 软件做转发，就可以看到 80 时钟周期，30-40ns 的转发延迟。其中，DPDK 省却了 Linux 协议栈 2000 时钟周期左右的额外操作，再配合全套硬件机制，发挥出 CPU 的最大能力。

这个 40ns，其实已经超越了很多网络硬件加速器的转发延迟。举个例子，Freescale 前几年基于 PowerPC 的网络处理器，在内部其实也支持 Cache Stashing。但是其硬件的转发延迟，却达到了 100ns。究其原因，是因为在使用了硬件加速器后，其中的队列管理相当复杂，CPU 为了拿到队列管理器分发给自己的包，需要设置 4 次以上的寄存器。并且，这些寄存器操作无法合并与并行，使得之前的 8 步中只需 20ns 的寄存器操作，变成了 80ns，超越 CPU 的数据操作延迟，成为整个流水中的瓶颈。结果在标准的纯转发测试中，硬件转发还不如 x86 软件来的快。

除了减少延迟，系统 DDR 带宽和随之而来的功耗也可以减少。Intel 曾经做过一个统计，如果对全部的包内容使用 cache，对于 256 字节（控制包）和 1518 字节（大包），分别可以减少 40-50%的带宽。如下图：

![img](https://pic4.zhimg.com/80/v2-b0835219d8108e6b564d6b8191a8ac8b_1440w.webp)

当然，由于芯片缓存大小和配置的不同，其他的芯片肯定会有不同结果。

包转发只是测试项中比较重要的一条。在加入加解密，压缩，关键字检测，各种查表等等操作后，整个转发延迟会增加至几千甚至上万时钟周期。这时候，硬件加速器的好处就显现出来了。也就是说，如果一个包进来按部就班的走完所有的加速器步骤，那能效比和性能面积比自然是加速器高。但是，通常情况下，很多功能都是用不到的，而这些不常用功能模块，自然也就是面积和成本。

整理完基本数据通路，我们来看看完整的网络加速器有哪些任务。以下是标准的 Ingress 通路上的处理流程：

![img](https://pic3.zhimg.com/80/v2-85ff118806f9a482c5da9e702d3f73be_1440w.webp)

主要是收包，分配和管理缓冲，数据搬移，字段检测和分类，队列管理和分发。

Egress 通路上的处理流程如下：

![img](https://pic3.zhimg.com/80/v2-50e51d43c5247fd957af3562eeb59ad2_1440w.webp)

主要是队列管理和分发，限速，分配和管理缓冲，数据搬移。

在 Ingress 和 Egress 共有的部分中，最复杂的模块是缓冲管理和队列管理，它们的任务分别如下：

缓冲管理：缓冲池的分配和释放；描述符的添加和删除；资源统计与报警；

队列管理：维护 MAC 到 CPU 的队列重映射表，入队和出队管理；包的保序和 QoS；CPU 缓存的预热，对齐；根据预设的分流策略，均衡每个 CPU 的负载

这些任务大部分都不是纯计算任务，而是一个硬件化的内存管理和队列管理。事实上，在新的网络加速器设计中，这些所谓的硬件化的部分，是由一系列专用小核加 C 代码完成的。因此，完全可以再进一步，去掉网络加速器，使用 SMP 的多组通用处理器来做纯软件的队列和缓冲管理，只保留存在大量计算或者简单操作的硬件模块挂在总线上。

再来看下之前的芯片模块，我们保留加解密和压缩引擎，去除队列和缓存管理 QB-Man，把 WRIOP 换成 A53 甚至更低端的小核，关键字检测可以用小核来实现。

网络处理上还有一些小的技术，如下：

Scattter Gather：前文提过，这可以处理分散的包头和负载，直接把多块数据 DMA 到最终的目的地，CPU 不用参与拷贝操作。这个在以太网控制器里可以轻易实现。

Checksum：用集成在以太网控制器里面的小模块对数据包进行校验和编码，每个包都需要做一次，可以用很小的代价来替代 CPU 的工作。

Interrupt aggregation：通过合并多个包的中断请求来降低中断响应的平均开销。在 Linux 和一些操作系统里面，响应中断和上下文切换需要毫秒级的时间，过多的中断导致真正处理包的时间变少。这个在以太网控制器里可以轻易实现。

Rate limiter：包发送限速，防止某个 MAC 口发送速度超过限速。这个用软件做比较难做的精确，而以太网控制器可以轻易计算单位时间内的发送字节数，容易实现。

Load balancing, Qos：之前主要是在阐述如何对单个包进行处理。对于多个包，由于网络包之间天然无依赖关系，所以很容易通过负载均衡把他们分发到各个处理器。那到底这个分发谁来做？专门分配一个核用软件做当然是可以的，还可以支持灵活的分发策略。但是这样一来，可能会需要额外的拷贝，Cache stashing 所减少的延迟优势也会消失。另外一个方法是在每一个以太网控制器里设置简单的队列（带优先级的多个 Ring buffer 之类的设计），和 CPU 号，线程 ID 或者虚拟机 ID 绑定。以太网控制器在对包头里的目的地址或者源地址做简单判断后，可以直接塞到相应的缓存或者 DDR 地址，供各个 CPU 来读取。如果有多个以太网控制器也没关系，每一个控制器针对每个 CPU 都可以配置不同的地址，让 CPU 上的软件来决定自己所属队列的读取策略，避免固化。

把这个技术用到 PCIe 的接口和网卡上，加入虚拟机 ID 的支持，就成了 srIOV。PCIe 控制器直接把以太网数据 DMA 到虚拟机所能看到的地址（同时也可使用 Cache stashing），不引起异常，提高效率。这需要虚拟机中所谓的穿透模式支持 IO（或者说外设）的虚拟化，在安全篇中有过介绍，此处就不再展开。同样的，SoC 内嵌的以太网处理器，在增加了系统 MMU 之后，同样也可以省略 PCIe 的接口，实现虚拟化，直接和虚拟机做数据高效传输。

![img](https://pic1.zhimg.com/80/v2-9713c0658fdacc4fe273863ee02c7ccc_1440w.webp)

解决了负载均衡的问题，接下去就要看怎么做包的处理了。有几种经典的方法：

第一是按照包来分，在一个核上做完某一特征数据包流所有的任务，然后发包。好处是包的数据就在本地缓存，坏处是如果所有代码都跑在一个核，所用到的参数可能过多，还是需要到 DDR 或者系统缓存去取；也有可能代码分支过多，CPU 里面的分支预测缓冲不够大，包处理程序再次回到某处代码时，还是会产生预测失败。

第二是按照任务的不同阶段来分，每个核做一部分任务，所有的包都流过每一个核。优缺点和上一条相反。

第三种就是前两者的混合，每一组核处理一个特定的数据流，组里再按照流水分阶段处理。这样可以根据业务代码来决定灵活性。

在以上的包处理程序中，一定会有一些操作是需要跨核的，比如对路由表的操作，统计数据的更新等。其中的一个重要原则就是：能不用锁就不要用锁，用了锁也需要使用效率最高的。

举个例子，如果需要实现一个计数器，统计 N 个 CPU 上最终处理过的包。首先，我们可以设计一个缓存行对齐的数据结构，包含若 N+1 个计数器，其中每一个计数器占据一行。每个 CPU 周期性的写入自己对应的那个计数器。由于计数器占据不同缓存行，所以写入操作不会引起额外的 Invalidation，也不会把更新过的数据踢到 DDR。最后，有一个独立的线程，不断读取前面 N 个计数器的值，累加到第 N+1 个计数器。在读取的时候，不会引起 Invalidation 和 eviction。此处不需要软件锁来防止多个线程对同一变量的操作，因为此处计数器的写和读并不需要严格的先后次序，只需要保证硬件一致性即可。

在不得不使用锁的时候，也要保证它的效率。软件上的各种实现这里就不讨论了，在硬件上，Cacheable 的锁变量及其操作最终会体现为两种形式：

第一， exclusive Access。原理是，对于一个缓存行里的变量，如果需要更新，必须先读然后写。当进行了一次读之后，进入锁定状态。期间如果有别的 CPU 对这个变量进行访问，锁定状态就会被取消，软件必须重复第一步；如果没有，就成功写入。这个原理被用来实现 spinlock 函数。根据硬件一致性协议 MESI，处在两个 CPU 中的线程，同时对某一个变量读写的话，数据会不断在各自的私有缓存或者共享缓存中来回传递，而不会保持在最快的一级缓存。这样显然降低效率。多个 CPU 组之间的 exclusive Access 效率更低。

第二， 原子操作。在 ARMv8.2 新的原子操作实现上，修改锁变量的操作可以在缓存控制器上实现，不需要送到 CPU 流水线。这个缓存可以使私有缓存，也可以是共享的系统缓存。和 exclusive access 相比较，省去了锁定状态失败后额外的轮询。不过这个需要系统总线和 CPU 的缓存支持本地更新操作，不然还是要反复读到每个 CPU 的流水线上才能更新数据。我在基础篇中举过例子，这种做法反而可能引起总线挂起，降低系统效率。

如果一定要跑 Linux 系统，那么一定要减少中断，内核切换，线程切换的时间。对应的，可以采用轮询，线程绑定 CPU 和用户态驱动来实现。此外，还可以采用大页表模式，减少页缺失的时间。这样，可以把标准 Linux 内核协议栈的 2000 左右时钟周期缩到几十纳秒，这也就是 DPDK 和商业网络转发软件效率高的原因。

最后稍稍谈下固态存储。企业级的固态硬盘存储芯片，芯片基本结构如下：

![img](https://pic3.zhimg.com/80/v2-f3f042069f9032e8910c4220195663c6_1440w.webp)

如上图，芯片分为前端和后端两块，前端处理来自 PCIe 的命令和数据，并把数据块映射到最终 nand 颗粒。同时，还要做负载均衡，磨损均衡，垃圾回收，阵列控制等工作，略过不表。这些都是可以用通用处理器来完成的，差不多每 1M 的随机 IOPS 需要 8 个跑在 1GHz 左右的 A7 处理器来支撑，每 TB 的 NAND 阵列，差不多需要 1GB 的 DDR 内存来存放映射表等信息。其余的一些压缩，加解密等，可以通过专用模块完成。后端要是作为 ONFI 的接口控制器，真正的从 NAND 里面读写数据，同时做 LDPC 生成和校验。

如果单看前端，会发现其实和网络处理非常像，查表，转发，负载均衡，都能找到对应的功能。不同的是，存储的应用非常固定，不需要错综复杂的业务流程，只需处理几种类型的命令和数据块就可以了。

一个存储系统的硬件成本，闪存才是大头，占据了 85%以上，还经常缺货。而闪存系统的毛利并不高，这也导致了固态存储控制器价格有限，哪怕是企业级的芯片，性能超出消费类几十倍，也无法像服务器那样拥有高利润。

一个潜在的解决方法，是把网络也集成到存储芯片中，并且跑 Linux。这样，固态硬盘由原来插在 x86 服务器上的一个 PCIe 设备，摇身一变成为一个网络节点，从而省掉了原来的服务器芯片及其内存（可别小看内存，它在一台服务器里面可以占到三分之一的成本）。

如上图，可以把 SPDK，DPDK，用户态驱动和用户态文件系统运行在一个芯片中，让一个网络包进来，不走内核网络协议栈和文件系统，直接从网络数据变成闪存上的文件块。在上层，运行类似 CEPH 之类的软件，把芯片实例化成一个网络存储节点。大致估算一下，这样的单芯片方案需要 16 核 A53 级别的处理器，运行在 1.5GHz 以下，在 28 纳米上，可以做到 5-10 瓦，远低于原来的服务器。

不过这个模式目前也存在着很大的问题。真正做存储芯片的公司，并不敢花大力气去推这种方案，因为它牵涉到了存储系统的基础架构变化，超出芯片公司的能力范畴，也不敢冒险去做这些改动。只有 OEM 和拥有大量数据中心互联网公司才能去定义，不过却苦于没有合适的单芯片方案。而且数据中心未必看得上省这点成本，毕竟成本大头在服务器芯片，内存以及机架电源。

### Reference

[1] https://zhuanlan.zhihu.com/p/34091597
