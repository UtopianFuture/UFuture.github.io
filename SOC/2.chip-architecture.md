## 芯片架构

捋顺了芯片的基础知识，现在终于可以开始攒机了。

首先，我们跑去 ARM，问它有没有现成的系统。ARM 说有啊，A73/G71/视频/显示/ISP/总线/系统控制/内存控制器/Trustzone 全都帮你集成好了，CPU 和 GPU 后端也做了，还是 16nm 的，包你性能和功耗不出问题。然后我们再跑到 Synopsys 或者 Cadence 买 EDA 工具，把仿真平台也一起打包了，顺带捎上周边 IP 和 PHY。至于基带，Wifi 和蓝牙，先不做吧，毕竟是第一次攒，要求不能太高。

在掏了几亿银子出来后，我们拿到一个系统框图：

![img](https://pic1.zhimg.com/80/v2-4b328d48474087b3c22a669c42eb8b05_1440w.webp?source=d16d100b)

A73MP4@3Ghz，A53MP4@1.6Ghz，G71MP8@850Mhz，显示 4K 分辨率双路输出，视频 4K，支持 VR，支持 Trustzone，内存带宽 25.6GB/s(LPDDR4x64@3200Gbps)。

本来可以乐呵呵的一手交钱一手交货的，可我们是来攒机的，不是买整机，我们得知道这个系统到底是怎么搭起来的，免得被坑。于是 ARM 给了一个更详细的图：

![img](https://pic1.zhimg.com/80/v2-d952d39eb872306a2e9ff7ccf25ffd1f_1440w.webp?source=d16d100b)

有了这张图，我们就可以对每个模块的性能，功耗，面积还有系统性能进行详细分析了。

首先是大核模块，这是一个手机芯片最受人关注的焦点。A73MP4 的 PPA 如下：

**以下数据经过修改，并不是真实数据。**

工艺：TSMC16FFLL+

频率：2.0GHz@SSG/0.72C/0C, 2.5GHz@TT/0.8V/85C，3GHz@TT/1.0C/85C

MP4 静态功耗：250mW@TT/0.8V/85C

动态功耗：200mW/Ghz@TT/0.8V/85C

面积：7mm，MP4, 64KB L1D, 2MB L2, No ECC, with NEON&FPU

跑分：835/Ghz SPECINT2K

也就是说，跑在极限 3Ghz 的时候，动态功耗是 200x1.25x1.25x3=937mW，四核得上 4W，而手机 SoC 最多也就能稳定跑在 3W。跑在 2.5G 时候，动态功耗是 200x2.5=500mW，MP4 总功耗 2.25W，可以接受。

A53 的如下：

**以下数据经过修改，并不是真实数据。**

工艺：TSMC16FFLL+

频率：1.5GHz@SSG/0.72C/0C, 1.6GHz@TT/0.8V/85C

MP4 静态功耗：60mW@TT/0.8V/85C

动态功耗：90mW/Ghz@TT/0.8V/85C

面积：4mm，MP4, 32KB L1D, 1MB L2, No ECC, with NEON&FP

跑分：500/Ghz SPECINT2K

四核跑在 1.6Ghz 时功耗 463 毫瓦，加上 A73MP4，一共 2.7 瓦。

这里的缓存大小是可以配置的，面积越大，性能收益其实是递减的，可以根据需要自行选取。

A53 的：

![img](https://pic1.zhimg.com/80/v2-aa865fcb4670e745303f555640a319de_1440w.webp?source=d16d100b)

A73 的：

![img](https://pica.zhimg.com/80/v2-d4174237045846ad0c359ea41f805ee6_1440w.webp?source=d16d100b)

作为 CPU，集成到 SoC 中的时候，一个重要的参数是访存延迟。有个数据，在 A73 上，每减少 5ns 的访存时间，SPECINT2K 分数就可以提高 1%。总的延迟如下：

![img](https://pic1.zhimg.com/80/v2-7c5e99475ac58d344680976024134b0e_1440w.webp?source=d16d100b)

上图只是一个例子，频率和参考设计并不一样。在上图，我们可以算出，访存在 CPU 之外，也就是总线和 DDR 上总共需要 58.7ns，这个时间也就是静态延时，在做系统设计时可以控制。要缩短它，一个是提高时钟频率，一个是减少路径上的模块。在 ARM 给的系统中总线使用了 CCI550，如果跑在 1Ghz，一个 Shareable 的传输从进去到 DDR 打个来回需要 10cycle，也就是 10ns（上图是 CCI500 跑在 800Mhz，所以是 12.5ns）。CCI550 和 CPU 之间还需要一个异步桥，来隔绝时钟，电源和电压域。这个异步桥其实也很花时间，来回需要 7.5ns，6 个 cycle，快赶上总线延迟了。但是没办法，它是省不掉的，只有通过增加总线频率来减少绝对延迟。只有一种情况例外，就是异步桥连接的两端时钟频率是整数倍，比如这里的内存控制器和 CCI 之间频率相同，那可以去掉（这里对着 CCI 手册看比较容易理解）。

有的设计会把 CCI550 以及它上面的 CPU，GPU 作为一个子网挂在 NoC 下，由 NoC 连到内存控制器。这样的好处是可以把交织和调度交给 NoC，坏处是凭空增加额外一层总线 10-20ns 的延迟，CPU 的跑分就要低了 2-4%。在 ARM 的方案中，交织由 CCI 直接完成，而调度交给内存控制器。既然所有主设备的访问都需要到 DDR，那由 DMC 来调度更合适。再加上没有采用两层总线的链接方式，一共可以去掉两层异步桥，省掉 12 个 cycle，已经快占到整个通路静态延迟的五分之一了。所以，现在我看到的主流总线拓扑，都是把 CCI 直接连内存控制器。那可不可以直接把 CPU 连内存控制器？只要 DMC 的端口够多，也没什么不可以，但是这样一来大小核就不能形成硬件支持的 SMP 了，功能上不允许。

路径上还有 DMC 和 PHY 的延迟，也是将近 15ns，20cycle，这部分挺难降低。如果实现 Trustzone，还要加上 DMC 和 CCI 之间的 TZC400 延迟，还会再多几个 cycle。至于 DDR 颗粒间的延迟（行选择，命令和预充电），可以通过准确的 DMC 预测和调度来减少，稍后再讲。

静态延迟算完，我们来看带宽和动态延迟。

CCI 和 CPU/GPU 之间是 ACE 口相连，数据宽度读写各 128bit，16Byte。如果总线跑在 800Mhz，单口的理论读或者写带宽分别是 12.8GB/s。这个速度够吗？可以通过 CPU 端和总线端的 Outstanding Transaction 个数来判断。A73 的手册上明确指出，ACE 接口同时支持 48 个 Cacheable 的读请求，14x4（CPU 核数量）设备和 non-Cacheable 的 TLB/指令读，7x4+16 写，这是固定的。而对应的 CCI550 的 ACE 接口支持 OT 数量是可配置的，配多大？有个简单公式可以计算。假设从 CPU 出来的传输全都是 64 字节的（Cacheable 的一定是，而 non-Cacheable 的未必），而我们的带宽是 12.8GB/s（假设是读），而一个传输从进入 ACE 到离开是 51.3ns（按照上图计算），那么最大 OT 数是 12.8x51.3/64=10.也就是说，只要 10 个 OT 就可以应付 CPU 那里 100 多个读的 OT，多了也没用，因为总线数据传输率在那。从这点看，瓶颈在总线的频率。

那为什么不通过增加总线的数据位宽来移除这个瓶颈呢？主要是是位宽增加，总线的最大频率相应的会降低，这是物理特性。同时我们可能会想，真的有需要做到更高的带宽吗，CPU 那里发的出来吗？我们可以计算一下。先看单个 A73 核，A73 是个乱序 CPU，它的读来自于三类方式，一个是读指令本身，一个是 PLD 指令，一个是一级缓存的预取。A73 的一级缓存支持 8 路预取，每路预取范围+/-32，深度是 4，预取请求会送到 PLD 单元一起执行，如下图。所以他们同时受到 BIU 上 Cacheable Linefill 数目的限制，也就是 8.此外还有一个隐含的瓶颈，就是 A73 的 LSU 有 12 个槽，读写共享，所有的读写请求（包括读写指令，PLD 和反馈过来的预取）都挂在这 12 个槽中独立维护。

![img](https://pic1.zhimg.com/80/v2-5fbaa39b765dd2183c7c52d3fcc26b1d_1440w.webp?source=d16d100b)

言归正传，假设系统里全是读请求，地址连续；此时 A73 每一条读指令的延迟是 3 个 cycle（PLD 可能只需要 1 个 cycle，我不确定）跑在 3Ghz 的 A73，一每秒可以发出 1G 次读，每个 8 字节（LDM）的话，就是 8GB/s 带宽，相当于每秒 128M 次 Linefill 请求，除以 BIU 的 8 个 Linefill 数量，也就是每个请求可以完成的时间是 60ns 这和我们看到的系统延迟接近。此时一级缓存预取也会起作用，所以 8 个 Linefill 肯定用满，瓶颈其实还是在在 BIU 的 Linefill 数量，按照 50ns 延迟算，差不多是 10GB/s。假设都是写，指令一个周期可以完成，每 cycle 写 8 字节（STM），带宽 24GB/s。如果写入的地址随机，会引起大量的 Linefill，于是又回到 BIU 的 Linefill 瓶颈。地址连续的话，会触发 Streaming 模式，在 BIU 内部只有一个传输 ID 号，由于有竞争，肯定是按次序的，所以显然这里会成为写的瓶颈。由于写的确认来自于二级缓存，而不是 DDR（是 cacheable 的数据，只是没分配一级缓存），延迟较小，假设是 8 个 cycle，每秒可以往外写 64B/8=8GB 的数据。总的来说，A73 核内部，瓶颈在 BIU。

到了二级缓存这，预取可以有 27 次。当一级缓存的 BIU 发现自己到了 4 次的极限，它可以告诉二级缓存去抓取（Hint）。单个核的时候，4+27<48，瓶颈是在一级缓存的 BIU；如果四个核同时发多路请求，那就是 4x8=32 个 OT，再加上每路都可以请求二级缓存，其总和大于 ACE 的 48 个 Cacheable 读请求。按照带宽算，每核是 10GB/s，然后 CPU 的 ACE 口的读是 48*64B/50ns=60GB/s。虽然 60GB/s 大于 4x10GB/s，看上去像是用不满，但是一级缓存会提示二级缓存自动去预取，所以，综合一级二级缓存，簇内的瓶颈还是在 ACE 接口上，也意味着总线和核之间的瓶颈在总线带宽上，并且 60GB/s 远高于 12.8GB/s。

A53 是顺序执行的，读取数据支持 miss-under-miss，没有 A73 上槽位的设计，就不具体分析了。

这里我简单计算下各个模块需要的带宽。

CPU 簇 x2，每个接口理论上总线接口共提供读写 43.2GB/s 带宽（我没有 SPECINT2K 时的带宽统计）。

G71MP8 在跑 Manhattan 时每一帧需要 370M 带宽（读加写未压缩时），850Mhz 可以跑到 45 帧，接近 17GB/s，压缩后需要 12GB/s。

4K 显示模块需要 4096x2160x4(Bytes)x60(帧)x4（图层）的输入，未压缩，共需要 8GB/s 的带宽，压缩后可能可以做到 5GB/s。这还都是单向输入的，没有计算反馈给其他模块的。

视频需求带宽少些，解压后是 2GB/s，加上解压过程当中对参考帧的访问，压缩后算 2GB/s。

还有 ISP 的，拍摄视频时会大些，算 2GB/s。

当然，以上这些模块不会全都同时运行，复杂的场景下，视频播放，GPU 渲染，CPU 跑驱动，显示模块也在工作，带宽需求可以达到 20GB/s。

而在参考设计里，我们的内存控制器物理极限是 3.2Gbpsx8Bytes=25.6GB/s,这还只是理论值，有些场景下系统设计不当，带宽利用率只有 70%，那只能提供 18GB/s 的带宽了，显然不够。

而我们也不能无限制的增加内存控制器和内存通道，一个是内存颗粒成本高，还有一个原因是功耗会随之上升。光是内存控制和 PHY，其功耗就可能超过 1 瓦。所以我们必须走提高带宽利用率的路线。

在这个前提下，我们需要分析下每个模块数据流的特征。

对于 CPU，先要保证它的延迟，然后再是带宽。

对于 GPU，需要保证它的带宽，然后再优化延迟，低延迟对性能跑分也是有提高的。

对于视频和 ISP，带宽相对来说不大，我们要保证它的实时性，间接的需要保证带宽。

对于显示模块，它的实时性要求更高，相对于应用跑的慢，跳屏可能更让人难以容忍。

其余的模块可以相对放在靠后的位置考虑。

在 CCI550 上，每个端口的带宽是读写共 21.6GB/s，大小核簇各需要一个端口，GPU 每四个核也需要一个端口。显示和视频并没有放到 CCI550，原因稍后解释。CCI550 的结构如下：

![img](https://picx.zhimg.com/80/v2-1a522a0d0b98295cef61852fb649f728_1440w.webp?source=d16d100b)

它一共可以有 7 个 ACE/ACE-Lite 进口，读写通道分开，地址共用，并且会进行竞争检查，每 cycle 可以仲裁 2 个地址请求。之前我们只计算了独立的读写通道带宽，那共用的地址会是瓶颈吗？算一下就知道。对于 CPU 和 GPU，所有从端口出来的传输，无论是不是 Cacheable 的，都不超过 64 字节，也就是 16x4 的突发。一拍地址对应四拍数据，那就是可以同时有八个端口发起传输，或者 4 个通道同时发起读和写。假设在最差情况下，CPU+GPU 同时发起 shareable 读请求，并且，读回去的数据全都引起了 eviction，造成同等数量的写。此时数据通道上不冲突，地址通道也正好符合。如果是 CPU+GPU 同时发起 shareable 写请求，并且全都命中别人的缓存，引起 invalidate，读通道此时空闲，但是 invalidate 占用地址，造成双倍地址请求，也符合上限。到 DMC 的地址不会成为瓶颈，因为共四个出口，每周期可以出 4 个。这里，我们使用的都是 shareable 传输，每次都会去 Snoop Filter 查找，每次可能需要两个对 TAG
RAM 的访问（一次判断，一次更新标志位，比如 Invalidate），那就是每 cycle 四次（地址 x2）。而我们的 Tagram 用的是 2cycle 访问延迟的 2 块 ram，也就是说，需要同时支持 8 个 OT。这一点，CCI550 已经做到了。在以上的计算中，DMC 是假设为固定延迟，并且 OT 足够，不成为瓶颈。实际情况中不会这么理想，光是带宽就不可能满足。

在 CCI550 中，有两处 OT 需要计算，一个是入口，每个端口独立，一个是做 Snooping 的时候，所有通道放在一起。之前我们计算过，如果静态延迟是 50ns，单口的读需要 10 个 OT。但是，上面的静态延迟算的是 DDR 命中一个打开的 bank，并且也不需要预充电。如果加上这两步，延迟就需要 85ns，而 CCI 必须为最差的情况作考虑。这样，OT 就变成了 12.8x85/64= 17。写的话需要的更少，因为写无需等待数据返回，只需要 DMC 给 Early response 就可以，也就是穿过 CCI550（10cycle）加上少于 DMC 的静态延迟（10cycle），不超过 20ns，OT 算 3。所以，单口 OT 就是 20，意味着需要 4x20=80 个来存储请求（四个入口）。

在出口，如果是 shareable 的传输，会需要去查表实现 snooping，此时所有的请求会放在一起，它的大小按照四个出口，每个带宽 2x12.8GB/s，共 100GB/s。但是，由于我们 DDR 最多也就 25.6GB/s，计算 OT 时候延迟按照读的 75ns（85-10，CCI 本身的延迟可以去掉了，写延迟小于读，按照读来计算），75x25.6/64=30。如果是 non-shareable 传输，那就还是使用入口的 OT。

上面计算出的 OT 都是用来存储读写请求的，其数据是要暂存于 CCI550 内部的 buffer。对于写来说如果数据不能暂存，那就没法给上一级 early response，减少上一层的 OT；对于读，可以在乱序地址访问时，提高效率。由于 DMC 会做一定程度的调度，所以 CCI550 发送到 DMC 的读写请求，很多时候不是按照读请求的次序完成的，需要额外的缓冲来存储先返回的数据。下面是读 buffer 大小和带宽的关系图。我们可以看到，对于随机地址，buffer 越深带宽越高。而对于顺序地址访问，几乎没有影响。这个深度根据队列原理是可以算出来的，我们这里就不写了。

![img](https://pica.zhimg.com/80/v2-4669dd62a30a9df973de25bd54b57f56_1440w.webp?source=d16d100b)

同样，增加缓冲也可以减少总线动态延迟，下图一目了然：

![img](https://picx.zhimg.com/80/v2-afac0b04f41947563906bdd859037861_1440w.webp?source=d16d100b)

至于设置缓冲的大小，根据队列原理，由于受到 DMC 调度的影响，数据回到 CCI500 的延迟期望值不容易计算，而直接设比较大的值又有些浪费（面积相对请求缓冲较大），比较靠谱的方法是跑仿真。

关于 CCI550 的资源配置，还有最后一项，即 tag ram 的大小。CCI550 的特点就是把各个主设备内部的缓存标志位记录下来提供给 Snooping 操作，那这个 tag ram 的大小该如何定。理论上，最大的配置是等同于各级 exclusive 缓存的 tag ram 总合。如果小于这个值，就需要一个操作，叫做 back invalidation，通知相应的缓存，把它内部的标志位去掉，免得标志位不一致。这个是由于 tag ram 本身的大小限制引入的操作，在执行这个操作过程中，同地址是有竞争的，这个竞争是多余的，并且会阻塞所有缓存对这一地址的 snooping 操作，所以需要尽量避免。ARM 定义了一个 CCI550 中的 tag ram 和原始大小的比例，使得 back invalidation 保持在 1-2%以下，参考下图：

![img](https://picx.zhimg.com/80/v2-5a28370dbec90acd1392e27ef4242ac3_1440w.webp?source=d16d100b)

简单来说，可以用所有 exclusive cache 总大小的 10%，把 back invalidation 限制在 1%以下。

其他的配置参数还有地址线宽度，决定了物理地址的范围，这个很容易理解。还有传输 ID 的宽度，太小的话没法支持足够的 OT。

到这里为止，我们已经设置好了硬件资源的大小，尽量使得动态延迟接近静态延迟。下图是全速运行时的带宽和功耗（这里移除了内存的瓶颈）。

![img](https://pica.zhimg.com/80/v2-884783830c98a1e98f35c78ce95a35da_1440w.webp?source=d16d100b)

![img](https://pic1.zhimg.com/80/v2-8b192e13d8b3dee10f76befd3ef8df2e_1440w.webp?source=d16d100b)

在这里，读写比例是 2：1，也就是说出口收到的请求最多 64+32=96GB/s。但我们可以看到有时是可以做到超越 100GB/s 的，为什么呢？因为有相当部分是命中了内部的 tag，转而从缓存进行读操作。我们之前算过，Snooping 操作在 CCI550 内部访问 tag ram 是足够的，但是如果命中，就需要从别人的缓存读数据，而这就收到上级缓存访问窗口的限制。所以我们看到当命中率是 100%，总带宽反而大大下降，此时的瓶颈在访问上级缓存。所幸的是，对于 SMP 的 CPU，通常命中率都在 10%以下，正处于带宽最高的那段。至于通用计算，是有可能超过 10%的，等到 GPU 之后再讨论。

第二副图显示了动态功耗，在 25GB/s 时为 110 毫瓦，还是远小于 CPU 的功耗，可以接受。静态功耗通常是 10-20 毫瓦，对于待机功耗其实并不低，而且 tag ram 是没法关闭的（300KB 左右），只要有一个小核在运作，就必须打开。所以 CCI550 的静态功耗会比 CCI400 高一些，这就是性能的代价，需要有部分关闭 tag ram 之类的设计来优化。

不过，一个没法避免的事实是，总线的入口带宽是 100GB/s，出口却只能是 25.6GB/s。我们需要一些方法来来达到理论带宽。在总线上，可以使用的方法有调度和交织。调度之前已经提到过，使用乱序来提高不存在竞争问题的读写传输，CCI550 自已有内嵌策略，不需要配置。

而交织在之前的文章中也提过。 在 CCI550 上，只接了 CPU 和 GPU，而他们发出的所有传输都不会超过 64 字节，也就是一个缓存行长度。只有一种可能的例外，就是 exclusive 传输，在 spin lock 中会用到。如果有一个变量跨缓存行，那么就有可能产生一个 128 字节的传输。而由于这类可能存在的传输，CCI550 可以设置的最小粒度是 128 字节，来避免它被拆开到两个内存控制器，破坏原子性。不过到目前为止，CPU/GPU 最多发出 64 字节的传输，不会有 128 字节，如果有跨缓存行的变量，直接切断并报异常。这样虽然会造成软件逻辑上的错误，但这错误是软件不遵守规范引起的，ARM 软件规范里明确禁止了这类问题。

对于视频和显示模块，它们并没有连在 CCI550。因为 CCI550 内部只有两个优先级，不利于 QoS。QoS 我们稍后再讨论，先看系统中的 NIC400 总线。参考设计中 NIC400 链接了视频/显示和 DMC，但其实 NIC400 并没有高效的交织功能，它的交织必须把整个物理地址空间按照 4KB 的页一个个写到配置文件。此外，它也没有调度和缓冲功能，所以在复杂系统中并不推荐使用。NIC400 更适合简单的系统，比如只有一个内存控制器，可以用更低的功耗和面积实现互联。在这里我们可以使用 NoC 来替换。NoC 还有一个适合传输长数据的功能，就是分割，可以把 265 字节甚至 4KB 的突发传输隔成小块，更利于交织。不过分割需要注意，必须把传输的标识符改了，并且要自己维护，并维持数据完整性。

这里就引出了一个问题，到底我们交织的粒度是多少合适？粒度越大，越不容易分散到各个 DMC，而粒度越小，越不容易形成对某个 DMC 和 DDR Bank 的连续访问。

![img](https://picx.zhimg.com/80/v2-c682aca71669523c938ce6fea4d6c98f_1440w.webp?source=d16d100b)

在四个通道的 DMC 上，CCI550 使用了这样一个哈希变换，来使得各个模块的传输能够平均分布：

Channel_Sel[0] = Addr[8] ^ Addr[10] ^ Addr [13] ^ Addr [21]

Channel_Sel[1] = Addr[9] ^ Addr [11] ^ Addr [16] ^ Addr[29]

然后在 DMC，使用了这样的变化来使访问分散到各个 Bank：

For four channel memory: Addr[29:0] = {Addr[31:10], Addr[7:0]}

经过哈希化后，在 CPU 的顺序地址上效果如下：

![img](https://picx.zhimg.com/80/v2-a2f60813f3c5a285f3a0fd85d30fc0e2_1440w.webp?source=d16d100b)

在显示模块的上效果如下：

![img](https://pic1.zhimg.com/80/v2-5887df27e1869ab13475a35a98719595_1440w.webp?source=d16d100b)

显然，在 256 字节的交织颗粒时效果最好，尤其是对于连续地址。对于随机地址，有个细节我们可以注意下，在颗粒度 64/128 字节的时候最好，这其实也符合预期。并且，由此还可以引出一个新的优化：对于随机访问多的地址段，我们使用 128 字节交织，而对于顺序访问多的地址段，使用 256 字节交织。但是请注意，这个编码必须对所有的主设备都保持一致，否则一定会出现数据一致性或者死锁问题。

刚刚我们提到了在 DMC 中的哈希优化。DMC 最重要的功能是调度，这是提高带宽利用率的关键。ARM 的 DMC 可以做到在多个主设备的 64 字节随机地址访问时做到 94%的利用率（读），写也可以到 88%。我们可以通过一些参数的设置来影响它的调度判断：读写操作切换时间，Rank（CS 信号）切换时间以及页内连续命中切换阈值，高优先超时切换时间等，这些看名字就能理解，不详细解释了。

调整完这么多参数，还有一个最根本的问题没有解决：DDR 理论带宽也只有 25.6GB/s，各个主设备一起访问，总会有拥挤的时候。这时该采取什么策略来最大程度上保证传输？答案是 QoS。QoS 的基本策略是设置优先级，其次还有一些辅助策略，比如动态优先级调整，整流等。

![img](https://pic1.zhimg.com/80/v2-400d6162ccdc3167239d09b573383b58_1440w.webp?source=d16d100b)

上图是优先等级表，左边是总线的（包括 CCI 和 NoC/NIC），高低依次是显示模块>视频>CPU>GPU>PCIe>DMA，基带跳过。在 CCI550 内部，实际上只有高和第两种优先级，CPU 高，GPU 低，没法区分更多的主设备。CCI550 的优先级高低是给外部用的，可以拿来给 DMC，让 DMC 定一个阈值，告诉 CCI 哪些级别之上优先发送，哪些暂缓。

当中的表是 DMC 内部对 QoS 优先级的重映射。DMC500 支持把某个模块的优先级提高，并动态调整。这里，CPU 在每秒发送 1.6GB 之后，优先级会被降低，和外部总线一样。下图中，我们可以看到在没有 QoS 的时候,CPU 的延迟最大（最左边 noqos），有了 QoS，由于 CPU 的优先级相对较高，延迟大大降低(qos_only)。而用了重映射之后，在 1.6GB 之下，延迟又进一步较大降低。

![img](https://picx.zhimg.com/80/v2-431337a0b4971029ba370ca99bec87d9_1440w.webp?source=d16d100b)

让我们再看一个例子，更好的说明如何通过调整优先级来在有限的带宽下保证实时性：

![img](https://picx.zhimg.com/80/v2-491bcc608646b06c44fc45e58da8ac81_1440w.webp?source=d16d100b)

在上图，我们看最右边，由于显示和视频总带宽需求不高，所以都能满足。

![img](https://picx.zhimg.com/80/v2-63d1672dac5ede88eedf847396356872_1440w.webp?source=d16d100b)

![img](https://pica.zhimg.com/80/v2-839b155f5a62cf836c9bf45c020d55cc_1440w.webp?source=d16d100b)

而这两张图里面，带宽需求上升，我们看到 Video 的带宽反而下降，而 CPU 不变。可在我们设置的表里，CPU 的优先级是低于视频模块的。于是我们利用 DMC 的门限，告诉 CCI 不要送低优先级的，并把 CPU 中小核的优先级降低到门限之下，这样只要有视频流，CCI 送到 DMC 的数据就会减少，如下图：

![img](https://picx.zhimg.com/80/v2-7b3f9a440eec28c8c20ac11a6d11530b_1440w.webp?source=d16d100b)

当然，我们也可以通过别的机制来达到类似的效果，比如限流等。攒机是个细致活，功夫够了肯定能调好，就是需要耐心和积累。

### 思考

好吧，前两篇文章作为入门还能看的懂一点，这篇文章就看不懂了。我认为是缺乏对 ARM 各种协议、产品的了解。亦或者是对芯片内部的各个模块没有清晰的认识。总的来说，ARM 提供的各种手册是第一手学习资料。

### Reference

[1] https://zhuanlan.zhihu.com/p/32366520
